--- ./kernel/sched/fair.c	2021-07-20 10:46:11.455524654 -0700
+++ ./kernel/sched/fair.c	2021-07-20 10:57:50.458961072 -0700
@@ -88,7 +88,11 @@
 unsigned int sysctl_sched_wakeup_granularity			= 500000UL;
 static unsigned int normalized_sysctl_sched_wakeup_granularity	= 500000UL;
 
+#ifdef CONFIG_CACULE_SCHED
+const_debug unsigned int sysctl_sched_migration_cost   = 200000UL;
+#else
 const_debug unsigned int sysctl_sched_migration_cost	= 250000UL;
+#endif
 
 #ifdef CONFIG_SMP
 /*
@@ -6587,6 +6591,26 @@
 	int want_affine = 0;
 	int sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);
 
+#ifdef CONFIG_CACULE_SCHED
+   struct sched_entity *se = &p->se;
+
+   if (!is_interactive(&se->cacule_node))
+       goto cfs_way;
+
+   // check first if the prev cpu
+   // has 0 tasks
+   if (cpumask_test_cpu(prev_cpu, p->cpus_ptr) &&
+       cpu_rq(prev_cpu)->cfs.nr_running == 0)
+       return prev_cpu;
+
+   new_cpu = find_least_IS_cpu(p);
+
+   if (new_cpu != -1)
+       return new_cpu;
+
+   new_cpu = prev_cpu;
+cfs_way:
+#else
 	if (sd_flag & SD_BALANCE_WAKE) {
 		record_wakee(p);
 
@@ -6713,6 +6737,7 @@
 }
 #endif /* CONFIG_SMP */
 
+#if !defined(CONFIG_CACULE_SCHED)
 static unsigned long wakeup_gran(struct sched_entity *se)
 {
 	unsigned long gran = sysctl_sched_wakeup_granularity;
@@ -6850,9 +6875,12 @@
 {
 	struct task_struct *curr = rq->curr;
 	struct sched_entity *se = &curr->se, *pse = &p->se;
+
+#if !defined(CONFIG_CACULE_SCHED)
 	struct cfs_rq *cfs_rq = task_cfs_rq(curr);
 	int scale = cfs_rq->nr_running >= sched_nr_latency;
 	int next_buddy_marked = 0;
+#endif /* CONFIG_CACULE_SCHED */
 
 	if (unlikely(se == pse))
 		return;
@@ -6866,6 +6894,7 @@
 	if (unlikely(throttled_hierarchy(cfs_rq_of(pse))))
 		return;
 
+#if !defined(CONFIG_CACULE_SCHED)
 	if (sched_feat(NEXT_BUDDY) && scale && !(wake_flags & WF_FORK)) {
 		set_next_buddy(pse);
 		next_buddy_marked = 1;
@@ -6900,7 +6929,12 @@
 	find_matching_se(&se, &pse);
 	update_curr(cfs_rq_of(se));
 	BUG_ON(!pse);
-	if (wakeup_preempt_entity(se, pse) == 1) {
+
+#ifdef CONFIG_CACULE_SCHED
+   if (entity_before(sched_clock(), &se->cacule_node, &pse->cacule_node) == 1)
+       goto preempt;
+#else
+   if (wakeup_preempt_entity(se, pse) == 1) {
 		/*
 		 * Bias pick_next to pick the sched entity that is
 		 * triggering this preemption.
@@ -6928,6 +6965,7 @@
 
 	if (sched_feat(LAST_BUDDY) && scale && entity_is_task(se))
 		set_last_buddy(se);
+#endif /* CONFIG_CACULE_SCHED */
 }
 
 static struct task_struct *
@@ -7097,7 +7135,10 @@
 {
 	struct task_struct *curr = rq->curr;
 	struct cfs_rq *cfs_rq = task_cfs_rq(curr);
+
+#if !defined(CONFIG_CACULE_SCHED)
 	struct sched_entity *se = &curr->se;
+#endif
 
 	/*
 	 * Are we the only task in the tree?
@@ -7105,7 +7146,9 @@
 	if (unlikely(rq->nr_running == 1))
 		return;
 
+#if !defined(CONFIG_CACULE_SCHED)
 	clear_buddies(cfs_rq, se);
+#endif
 
 	if (curr->policy != SCHED_BATCH) {
 		update_rq_clock(rq);
@@ -7121,7 +7164,9 @@
 		rq_clock_skip_update(rq);
 	}
 
+#if !defined(CONFIG_CACULE_SCHED)
 	set_skip_buddy(se);
+#endif
 }
 
 static bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt)
@@ -7132,8 +7177,10 @@
 	if (!se->on_rq || throttled_hierarchy(cfs_rq_of(se)))
 		return false;
 
+#if !defined(CONFIG_CACULE_SCHED)
 	/* Tell the scheduler that we'd really like pse to run next. */
 	set_next_buddy(se);
+#endif
 
 	yield_task_fair(rq);
 
