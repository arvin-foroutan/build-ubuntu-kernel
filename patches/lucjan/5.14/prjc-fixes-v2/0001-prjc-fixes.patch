From 5d47f165f17e5c7d423b8ec46b3eeac14ab4cc87 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 2 Sep 2021 02:08:16 +0200
Subject: [PATCH 1/2] sched/alt: Fix fails on x86 UP build, second attempt

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 kernel/sched/alt_core.c  | 24 ++++++++++++++++++++++++
 kernel/sched/alt_sched.h | 30 ++++++++++++++++++++++++++++++
 2 files changed, 54 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 5df1157a5..719c87d69 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -492,6 +492,30 @@ rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
 	raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
 }
 
+void raw_spin_rq_lock_nested(struct rq *rq, int subclass)
+{
+       raw_spinlock_t *lock;
+
+       /* Matches synchronize_rcu() in __sched_core_enable() */
+       preempt_disable();
+
+       for (;;) {
+               lock = __rq_lockp(rq);
+               raw_spin_lock_nested(lock, subclass);
+               if (likely(lock == __rq_lockp(rq))) {
+                       /* preempt_count *MUST* be > 1 */
+                       preempt_enable_no_resched();
+                       return;
+               }
+               raw_spin_unlock(lock);
+       }
+}
+
+void raw_spin_rq_unlock(struct rq *rq)
+{
+       raw_spin_unlock(rq_lockp(rq));
+}
+
 /*
  * RQ-clock updating methods:
  */
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 7a4880955..935ed6ab9 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -443,6 +443,36 @@ this_rq_lock_irq(struct rq_flags *rf)
 	return rq;
 }
 
+extern void raw_spin_rq_lock_nested(struct rq *rq, int subclass);
+extern void raw_spin_rq_unlock(struct rq *rq);
+
+static inline raw_spinlock_t *__rq_lockp(struct rq *rq)
+{
+       return &rq->lock;
+}
+
+static inline raw_spinlock_t *rq_lockp(struct rq *rq)
+{
+       return __rq_lockp(rq);
+}
+
+static inline void raw_spin_rq_lock(struct rq *rq)
+{
+       raw_spin_rq_lock_nested(rq, 0);
+}
+
+static inline void raw_spin_rq_lock_irq(struct rq *rq)
+{
+       local_irq_disable();
+       raw_spin_rq_lock(rq);
+}
+
+static inline void raw_spin_rq_unlock_irq(struct rq *rq)
+{
+       raw_spin_rq_unlock(rq);
+       local_irq_enable();
+}
+
 static inline int task_current(struct rq *rq, struct task_struct *p)
 {
 	return rq->curr == p;
-- 
2.33.0


From a6163fc6a28605046d7ef207a6314273edf970cd Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 2 Sep 2021 13:52:19 +0200
Subject: [PATCH 2/2] sched/alt: disable sched_core when sched_alt is enabled

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 kernel/Kconfig.preempt | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index 5876e30c5..7594d0a31 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -102,7 +102,7 @@ config PREEMPT_DYNAMIC
 
 config SCHED_CORE
 	bool "Core Scheduling for SMT"
-	depends on SCHED_SMT
+	depends on SCHED_SMT && !SCHED_ALT
 	help
 	  This option permits Core Scheduling, a means of coordinated task
 	  selection across SMT siblings. When enabled -- see
-- 
2.33.0

