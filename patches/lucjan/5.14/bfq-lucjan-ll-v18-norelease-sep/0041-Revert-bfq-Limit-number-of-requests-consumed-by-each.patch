From af4003f0b7ba57aee1dff1af9b0cbb87e4a7f3de Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Wed, 6 Oct 2021 22:26:05 +0200
Subject: [PATCH 41/71] Revert "bfq: Limit number of requests consumed by each
 cgroup"

This reverts commit 31274e7204c30bf35b7b830586e2531fd9d9c6b4.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/bfq-iosched.c | 103 ++++++++------------------------------------
 1 file changed, 18 insertions(+), 85 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 65d9c47e1..6a73b6494 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -575,71 +575,6 @@ static struct request *bfq_choose_req(struct bfq_data *bfqd,
 	}
 }
 
-#define BFQ_LIMIT_INLINE_DEPTH 16
-
-#ifdef CONFIG_BFQ_GROUP_IOSCHED
-static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
-{
-	struct bfq_data *bfqd = bfqq->bfqd;
-	struct bfq_entity *entity = &bfqq->entity;
-	struct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];
-	struct bfq_entity **entities = inline_entities;
-	int depth, level;
-	bool ret = false;
-
-	if (!entity->on_st_or_in_serv)
-		return false;
-
-	/* +1 for bfqq entity, root cgroup not included */
-	depth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;
-	if (depth > BFQ_LIMIT_INLINE_DEPTH) {
-		entities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);
-		if (!entities)
-			return false;
-	}
-
-	spin_lock_irq(&bfqd->lock);
-	if (!entity->on_st_or_in_serv)
-		goto out;
-	/* Gather our ancestors as we need to traverse them in reverse order */
-	level = 0;
-	for_each_entity(entity) {
-		/* Uh, more parents than cgroup subsystem thinks? */
-		if (WARN_ON_ONCE(level >= depth))
-			break;
-		entities[level++] = entity;
-	}
-	WARN_ON_ONCE(level != depth);
-	for (level--; level >= 0; level--) {
-		entity = entities[level];
-		/*
-		 * If the leaf entity has work to do, parents should be tracked
-		 * as well.
-		 */
-		WARN_ON_ONCE(!entity->on_st_or_in_serv);
-		limit = DIV_ROUND_CLOSEST(limit * entity->weight,
-					bfq_entity_service_tree(entity)->wsum);
-		if (entity->allocated >= limit) {
-			bfq_log_bfqq(bfqq->bfqd, bfqq,
-				"too many requests: allocated %d limit %d level %d",
-				entity->allocated, limit, level);
-			ret = true;
-			break;
-		}
-	}
-out:
-	spin_unlock_irq(&bfqd->lock);
-	if (entities != inline_entities)
-		kfree(entities);
-	return ret;
-}
-#else
-static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
-{
-	return false;
-}
-#endif
-
 /*
  * Async I/O can easily starve sync I/O (both sync reads and sync
  * writes), by consuming all tags. Similarly, storms of sync writes,
@@ -650,28 +585,16 @@ static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
 static void bfq_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 {
 	struct bfq_data *bfqd = data->q->elevator->elevator_data;
-	struct bfq_io_cq *bic = data->icq ? icq_to_bic(data->icq) : NULL;
-	struct bfq_queue *bfqq = bic ? bic_to_bfqq(bic, op_is_sync(op)) : NULL;
-	int depth;
 
-	/* Sync reads have full depth available */
 	if (op_is_sync(op) && !op_is_write(op))
-		depth = 0;
-	else
-		depth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(op)];
+		return;
 
-	/*
-	 * Does queue (or any parent entity) exceed number of requests that
-	 * should be available to it? Heavily limit depth so that it cannot
-	 * consume more available requests and thus starve other entities.
-	 */
-	if (bfqq && bfqq_request_over_limit(bfqq, data->q->nr_requests))
-		depth = 1;
+	data->shallow_depth =
+		bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(op)];
 
 	bfq_log(bfqd, "[%s] wr_busy %d sync %d depth %u",
-		__func__, bfqd->wr_busy_queues, op_is_sync(op), depth);
-	if (depth)
-		data->shallow_depth = depth;
+			__func__, bfqd->wr_busy_queues, op_is_sync(op),
+			data->shallow_depth);
 }
 
 static struct bfq_queue *
@@ -6988,8 +6911,11 @@ void bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)
  * See the comments on bfq_limit_depth for the purpose of
  * the depths set in the function. Return minimum shallow depth we'll use.
  */
-static void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)
+static unsigned int bfq_update_depths(struct bfq_data *bfqd,
+				      struct sbitmap_queue *bt)
 {
+	unsigned int i, j, min_shallow = UINT_MAX;
+
 	/*
 	 * In-word depths if no bfq_queue is being weight-raised:
 	 * leaving 25% of tags only for sync reads.
@@ -7020,15 +6946,22 @@ static void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)
 	bfqd->word_depths[1][0] = max(((1U << bt->sb.shift) * 3) >> 4, 1U);
 	/* no more than ~37% of tags for sync writes (~20% extra tags) */
 	bfqd->word_depths[1][1] = max(((1U << bt->sb.shift) * 6) >> 4, 1U);
+
+	for (i = 0; i < 2; i++)
+		for (j = 0; j < 2; j++)
+			min_shallow = min(min_shallow, bfqd->word_depths[i][j]);
+
+	return min_shallow;
 }
 
 static void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)
 {
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 	struct blk_mq_tags *tags = hctx->sched_tags;
+	unsigned int min_shallow;
 
-	bfq_update_depths(bfqd, tags->bitmap_tags);
-	sbitmap_queue_min_shallow_depth(tags->bitmap_tags, 1);
+	min_shallow = bfq_update_depths(bfqd, tags->bitmap_tags);
+	sbitmap_queue_min_shallow_depth(tags->bitmap_tags, min_shallow);
 }
 
 static int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)
-- 
2.33.0.610.gcefe983a32

