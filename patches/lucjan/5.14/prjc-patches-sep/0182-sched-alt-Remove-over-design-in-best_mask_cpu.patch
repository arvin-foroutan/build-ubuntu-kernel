From 2d5b182ab1917c7c85896046afd5e0ebdfa1d3f5 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 10 Jul 2021 15:28:11 +0000
Subject: [PATCH 182/204] sched/alt: Remove over design in best_mask_cpu()

---
 kernel/sched/alt_core.c  | 30 +++++++--------------
 kernel/sched/alt_sched.h | 58 ++++------------------------------------
 2 files changed, 15 insertions(+), 73 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 472d73646b67..38f2ef489b37 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -207,8 +207,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 
 		cpumask_and(&tmp, cpu_smt_mask(cpu), sched_rq_watermark);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
-			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
-				   &sched_sg_idle_mask);
+			cpumask_or(&sched_sg_idle_mask,
+				   &sched_sg_idle_mask, cpu_smt_mask(cpu));
 	}
 #endif
 }
@@ -3528,8 +3528,7 @@ static inline int active_load_balance_cpu_stop(void *data)
 	    cpumask_and(&tmp, p->cpus_ptr, &sched_sg_idle_mask) &&
 	    !is_migration_disabled(p)) {
 		int cpu = cpu_of(rq);
-		int dcpu = __best_mask_cpu(cpu, &tmp,
-					   per_cpu(sched_cpu_llc_mask, cpu));
+		int dcpu = __best_mask_cpu(&tmp, per_cpu(sched_cpu_llc_mask, cpu));
 		rq = move_queued_task(rq, p, dcpu);
 	}
 
@@ -3573,34 +3572,25 @@ static inline int sg_balance_trigger(const int cpu)
 static inline void sg_balance_check(struct rq *rq)
 {
 	cpumask_t chk;
-	int cpu;
-
-	/* exit when no sg in idle */
-	if (cpumask_empty(&sched_sg_idle_mask))
-		return;
+	int cpu = cpu_of(rq);
 
 	/* exit when cpu is offline */
 	if (unlikely(!rq->online))
 		return;
 
-	cpu = cpu_of(rq);
 	/*
 	 * Only cpu in slibing idle group will do the checking and then
 	 * find potential cpus which can migrate the current running task
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
-	    cpumask_andnot(&chk, cpu_online_mask, &sched_rq_pending_mask) &&
-	    cpumask_andnot(&chk, &chk, sched_rq_watermark)) {
-		int i, tried = 0;
+	    cpumask_andnot(&chk, cpu_online_mask, sched_rq_watermark) &&
+	    cpumask_andnot(&chk, &chk, &sched_rq_pending_mask)) {
+		int i;
 
 		for_each_cpu_wrap(i, &chk, cpu) {
-			if (cpumask_subset(cpu_smt_mask(i), &chk)) {
-				if (sg_balance_trigger(i))
-					return;
-				if (tried)
-					return;
-				tried++;
-			}
+			if (cpumask_subset(cpu_smt_mask(i), &chk) &&
+			    sg_balance_trigger(i))
+				return;
 		}
 	}
 }
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index f9f79422bf0e..7a48809550bf 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -302,68 +302,20 @@ enum {
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
-static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
-				  const cpumask_t *mask)
+static inline int
+__best_mask_cpu(const cpumask_t *cpumask, const cpumask_t *mask)
 {
-#if NR_CPUS <= 64
-	unsigned long t;
+	int cpu;
 
-	while ((t = cpumask->bits[0] & mask->bits[0]) == 0UL)
-		mask++;
-
-	return __ffs(t);
-#else
 	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
 		mask++;
+
 	return cpu;
-#endif
 }
 
 static inline int best_mask_cpu(int cpu, const cpumask_t *mask)
 {
-#if NR_CPUS <= 64
-	unsigned long llc_match;
-	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
-
-	if ((llc_match = mask->bits[0] & chk->bits[0])) {
-		unsigned long match;
-
-		chk = per_cpu(sched_cpu_topo_masks, cpu);
-		if (mask->bits[0] & chk->bits[0])
-			return cpu;
-
-#ifdef CONFIG_SCHED_SMT
-		chk++;
-		if ((match = mask->bits[0] & chk->bits[0]))
-			return __ffs(match);
-#endif
-
-		return __ffs(llc_match);
-	}
-
-	return __best_mask_cpu(cpu, mask, chk + 1);
-#else
-	cpumask_t llc_match;
-	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
-
-	if (cpumask_and(&llc_match, mask, chk)) {
-		cpumask_t tmp;
-
-		chk = per_cpu(sched_cpu_topo_masks, cpu);
-		if (cpumask_test_cpu(cpu, mask))
-			return cpu;
-
-#ifdef CONFIG_SCHED_SMT
-		chk++;
-		if (cpumask_and(&tmp, mask, chk))
-			return cpumask_any(&tmp);
-#endif
-
-		return cpumask_any(&llc_match);
-	}
-
-	return __best_mask_cpu(cpu, mask, chk + 1);
-#endif
+	return __best_mask_cpu(mask, per_cpu(sched_cpu_topo_masks, cpu));
 }
 
 extern void flush_smp_call_function_from_idle(void);
-- 
2.33.0

