From 0bda34ff2eb78b5028719b4d307a06d30c9e8ae9 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 14:49:46 +0000
Subject: [PATCH 155/204] sched/alt: Rename BMQ&PDS implement files.

---
 kernel/sched/alt_core.c  |   4 +-
 kernel/sched/alt_sched.h |   9 +-
 kernel/sched/bmq.h       | 206 +++++++++++++++++++++++++-
 kernel/sched/bmq_imp.h   | 203 --------------------------
 kernel/sched/pds.h       | 303 ++++++++++++++++++++++++++++++++++++++-
 kernel/sched/pds_imp.h   | 300 --------------------------------------
 6 files changed, 506 insertions(+), 519 deletions(-)
 delete mode 100644 kernel/sched/bmq_imp.h
 delete mode 100644 kernel/sched/pds_imp.h

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b1c17ff1642c..9ade1b64aa9c 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -143,10 +143,10 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
 #ifdef CONFIG_SCHED_BMQ
-#include "bmq_imp.h"
+#include "bmq.h"
 #endif
 #ifdef CONFIG_SCHED_PDS
-#include "pds_imp.h"
+#include "pds.h"
 #endif
 
 static inline void update_sched_rq_watermark(struct rq *rq)
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 21f359102fbc..58ff6212b446 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -50,12 +50,17 @@
 #include <trace/events/sched.h>
 
 #ifdef CONFIG_SCHED_BMQ
-#include "bmq.h"
+/* bits:
+ * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #endif
 #ifdef CONFIG_SCHED_PDS
-#include "pds.h"
+/* bits: RT(0-99), nice width / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
 #endif
 
+#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+
 #ifdef CONFIG_SCHED_DEBUG
 # define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
 extern void resched_latency_warn(int cpu, u64 latency);
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7f83b7c42619..f6bd3421b95c 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -1,9 +1,203 @@
-#ifndef BMQ_H
-#define BMQ_H
+#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-/* bits:
- * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
-#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+/*
+ * BMQ only routines
+ */
+#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
+#define boost_threshold(p)	(sched_timeslice_ns >>\
+				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
 
+static inline void boost_task(struct task_struct *p)
+{
+	int limit;
+
+	switch (p->policy) {
+	case SCHED_NORMAL:
+		limit = -MAX_PRIORITY_ADJ;
+		break;
+	case SCHED_BATCH:
+	case SCHED_IDLE:
+		limit = 0;
+		break;
+	default:
+		return;
+	}
+
+	if (p->boost_prio > limit)
+		p->boost_prio--;
+}
+
+static inline void deboost_task(struct task_struct *p)
+{
+	if (p->boost_prio < MAX_PRIORITY_ADJ)
+		p->boost_prio++;
+}
+
+/*
+ * Common interfaces
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	if (task_has_rt_policy(p))
+		return MAX_RT_PRIO - 1 - p->rt_priority;
+
+	return p->static_prio + MAX_PRIORITY_ADJ;
+}
+
+static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	p->time_slice = sched_timeslice_ns;
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
+		if (SCHED_RR != p->policy)
+			deboost_task(p);
+		requeue_task(p, rq);
+	}
+}
+
+inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
+static inline void sched_queue_init(struct rq *rq)
+{
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	struct sched_queue *q = &rq->queue;
+
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
+/*
+ * This routine used in bmq scheduler only which assume the idle task in the bmq
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[idx];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
+		head = &rq->queue.heads[idx];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
+	sched_info_dequeued(rq, p);			\
+							\
+	list_del(&p->sq_node);				\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
+		clear_bit(p->sq_idx, rq->queue.bitmap);\
+		func;					\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(p->sq_idx, rq->queue.bitmap)
+
+#define __SCHED_REQUEUE_TASK(p, rq, func)				\
+{									\
+	int idx = task_sched_prio(p, rq);				\
+\
+	list_del(&p->sq_node);						\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
+	if (idx != p->sq_idx) {					\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
+			clear_bit(p->sq_idx, rq->queue.bitmap);	\
+		p->sq_idx = idx;					\
+		set_bit(p->sq_idx, rq->queue.bitmap);			\
+		func;							\
+	}								\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	return (task_sched_prio(p, rq) != p->sq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = (p->boost_prio < 0) ?
+		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice/boost
+ *
+ * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+	return (p->prio - MAX_RT_PRIO + p->boost_prio);
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = MAX_PRIORITY_ADJ;
+}
+
+#ifdef CONFIG_SMP
+static void sched_task_ttwu(struct task_struct *p)
+{
+	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
+		boost_task(p);
+}
 #endif
+
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
+{
+	if (rq_switch_time(rq) < boost_threshold(p))
+		boost_task(p);
+}
+
+static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
deleted file mode 100644
index f6bd3421b95c..000000000000
--- a/kernel/sched/bmq_imp.h
+++ /dev/null
@@ -1,203 +0,0 @@
-#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
-
-/*
- * BMQ only routines
- */
-#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
-
-static inline void boost_task(struct task_struct *p)
-{
-	int limit;
-
-	switch (p->policy) {
-	case SCHED_NORMAL:
-		limit = -MAX_PRIORITY_ADJ;
-		break;
-	case SCHED_BATCH:
-	case SCHED_IDLE:
-		limit = 0;
-		break;
-	default:
-		return;
-	}
-
-	if (p->boost_prio > limit)
-		p->boost_prio--;
-}
-
-static inline void deboost_task(struct task_struct *p)
-{
-	if (p->boost_prio < MAX_PRIORITY_ADJ)
-		p->boost_prio++;
-}
-
-/*
- * Common interfaces
- */
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return p->static_prio + MAX_PRIORITY_ADJ;
-}
-
-static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
-{
-	p->time_slice = sched_timeslice_ns;
-
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
-		if (SCHED_RR != p->policy)
-			deboost_task(p);
-		requeue_task(p, rq);
-	}
-}
-
-inline int task_running_nice(struct task_struct *p)
-{
-	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
-}
-
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
-static inline void sched_queue_init(struct rq *rq)
-{
-	struct sched_queue *q = &rq->queue;
-	int i;
-
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
-/*
- * This routine used in bmq scheduler only which assume the idle task in the bmq
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
-	const struct list_head *head = &rq->queue.heads[idx];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
-		head = &rq->queue.heads[idx];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
-	sched_info_dequeued(rq, p);			\
-							\
-	list_del(&p->sq_node);				\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
-		clear_bit(p->sq_idx, rq->queue.bitmap);\
-		func;					\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(p->sq_idx, rq->queue.bitmap)
-
-#define __SCHED_REQUEUE_TASK(p, rq, func)				\
-{									\
-	int idx = task_sched_prio(p, rq);				\
-\
-	list_del(&p->sq_node);						\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->sq_idx) {					\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
-			clear_bit(p->sq_idx, rq->queue.bitmap);	\
-		p->sq_idx = idx;					\
-		set_bit(p->sq_idx, rq->queue.bitmap);			\
-		func;							\
-	}								\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio(p, rq) != p->sq_idx);
-}
-
-static void sched_task_fork(struct task_struct *p, struct rq *rq)
-{
-	p->boost_prio = (p->boost_prio < 0) ?
-		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
-}
-
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice/boost
- *
- * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-{
-	p->boost_prio = MAX_PRIORITY_ADJ;
-}
-
-#ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p)
-{
-	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
-		boost_task(p);
-}
-#endif
-
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
-{
-	if (rq_switch_time(rq) < boost_threshold(p))
-		boost_task(p);
-}
-
-static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 623908cf4380..8cc656a7cc48 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,9 +1,300 @@
-#ifndef PDS_H
-#define PDS_H
+#define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-/* bits:
- * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
-#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+static const u64 user_prio2deadline[NICE_WIDTH] = {
+/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
+/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
+/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
+/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
+/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
+/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
+/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
+/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
+};
 
+#define SCHED_PRIO_SLOT		(4ULL << 20)
+#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
+
+static inline int normal_prio(struct task_struct *p)
+{
+	if (task_has_rt_policy(p))
+		return MAX_RT_PRIO - 1 - p->rt_priority;
+
+	return MAX_RT_PRIO;
+}
+
+extern int alt_debug[20];
+
+static inline int
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
+{
+	int delta;
+
+	delta = rq->time_edge + 20 - (p->deadline >> 23);
+	if (delta < 0) {
+		delta = 0;
+		alt_debug[0]++;
+	}
+	delta = 19 - min(delta, 19);
+
+	return delta;
+}
+
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
+
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
+
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO +
+		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
+}
+
+int task_running_nice(struct task_struct *p)
+{
+	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
+}
+
+DECLARE_BITMAP(normal_mask, SCHED_BITS);
+
+static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
+{
+	DECLARE_BITMAP(normal, SCHED_BITS);
+
+	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
+	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
+	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
+
+	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
+	bitmap_or(mask, mask, normal, SCHED_BITS);
+}
+
+static inline void update_rq_time_edge(struct rq *rq)
+{
+	struct list_head head;
+	u64 old = rq->time_edge;
+	u64 now = rq->clock >> 23;
+	u64 prio, delta;
+
+	if (now == old)
+		return;
+
+	delta = min(20ULL, now - old);
+	INIT_LIST_HEAD(&head);
+
+	prio = MAX_RT_PRIO;
+	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
+		u64 idx;
+
+		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
+		list_splice_tail_init(rq->queue.heads + idx, &head);
+	}
+	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
+	rq->time_edge = now;
+	if (!list_empty(&head)) {
+		struct task_struct *p;
+
+		list_for_each_entry(p, &head, sq_node)
+			p->sq_idx = MAX_RT_PRIO + now % 20;
+
+		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
+		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
+	}
+}
+
+static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
+{
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock +
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
+	p->time_slice = sched_timeslice_ns;
+	sched_renew_deadline(p, rq);
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq);
+}
+
+/*
+ * Init the queue structure in rq
+ */
+static inline void sched_queue_init(struct rq *rq)
+{
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	struct sched_queue *q = &rq->queue;
+	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
+
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
+static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+}
+
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
+}
+
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+
+/*
+ * Requeue a task @p to @rq
+ */
+#define __SCHED_REQUEUE_TASK(p, rq, func)					\
+{\
+	int idx = task_sched_prio_idx(p, rq);					\
+\
+	list_del(&p->sq_node);							\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
+	if (idx != p->sq_idx) {						\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
+			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+		p->sq_idx = idx;						\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+		func;								\
+	}									\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	return (task_sched_prio_idx(p, rq) != p->sq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
+{
+	sched_renew_deadline(p, rq);
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice
+ *
+ * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	int ret;
+
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+
+	/*preempt_disable();
+	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
+	ret = p->static_prio - MAX_RT_PRIO;
+	/*preempt_enable();*/
+
+	return ret;
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	time_slice_expired(p, rq);
+}
+
+#ifdef CONFIG_SMP
+static void sched_task_ttwu(struct task_struct *p) {}
 #endif
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
deleted file mode 100644
index 8cc656a7cc48..000000000000
--- a/kernel/sched/pds_imp.h
+++ /dev/null
@@ -1,300 +0,0 @@
-#define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
-
-static const u64 user_prio2deadline[NICE_WIDTH] = {
-/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
-/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
-/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
-/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
-/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
-/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
-/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
-/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
-};
-
-#define SCHED_PRIO_SLOT		(4ULL << 20)
-#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
-
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return MAX_RT_PRIO;
-}
-
-extern int alt_debug[20];
-
-static inline int
-task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
-{
-	int delta;
-
-	delta = rq->time_edge + 20 - (p->deadline >> 23);
-	if (delta < 0) {
-		delta = 0;
-		alt_debug[0]++;
-	}
-	delta = 19 - min(delta, 19);
-
-	return delta;
-}
-
-static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
-{
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
-
-	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
-}
-
-static inline int
-task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
-{
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
-
-	return MAX_RT_PRIO +
-		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
-}
-
-int task_running_nice(struct task_struct *p)
-{
-	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
-}
-
-DECLARE_BITMAP(normal_mask, SCHED_BITS);
-
-static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
-{
-	DECLARE_BITMAP(normal, SCHED_BITS);
-
-	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
-	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
-	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
-
-	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
-	bitmap_or(mask, mask, normal, SCHED_BITS);
-}
-
-static inline void update_rq_time_edge(struct rq *rq)
-{
-	struct list_head head;
-	u64 old = rq->time_edge;
-	u64 now = rq->clock >> 23;
-	u64 prio, delta;
-
-	if (now == old)
-		return;
-
-	delta = min(20ULL, now - old);
-	INIT_LIST_HEAD(&head);
-
-	prio = MAX_RT_PRIO;
-	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
-		u64 idx;
-
-		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
-		list_splice_tail_init(rq->queue.heads + idx, &head);
-	}
-	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
-	rq->time_edge = now;
-	if (!list_empty(&head)) {
-		struct task_struct *p;
-
-		list_for_each_entry(p, &head, sq_node)
-			p->sq_idx = MAX_RT_PRIO + now % 20;
-
-		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
-		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
-	}
-}
-
-static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
-{
-	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
-{
-	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
-	p->time_slice = sched_timeslice_ns;
-	sched_renew_deadline(p, rq);
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
-		requeue_task(p, rq);
-}
-
-/*
- * Init the queue structure in rq
- */
-static inline void sched_queue_init(struct rq *rq)
-{
-	struct sched_queue *q = &rq->queue;
-	int i;
-
-	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-/*
- * Init idle task and put into queue structure of rq
- * IMPORTANT: may be called multiple times for a single cpu
- */
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
-static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
-}
-
-static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
-}
-
-/*
- * This routine assume that the idle task always in queue
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
-	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
-				    sched_idx2prio(idx, rq) + 1);
-		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
-	sched_info_dequeued(rq, p);				\
-								\
-	list_del(&p->sq_node);					\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
-		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
-			  rq->queue.bitmap);			\
-		func;						\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio_idx(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
-
-/*
- * Requeue a task @p to @rq
- */
-#define __SCHED_REQUEUE_TASK(p, rq, func)					\
-{\
-	int idx = task_sched_prio_idx(p, rq);					\
-\
-	list_del(&p->sq_node);							\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {						\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
-		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
-		func;								\
-	}									\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio_idx(p, rq) != p->sq_idx);
-}
-
-static void sched_task_fork(struct task_struct *p, struct rq *rq)
-{
-	sched_renew_deadline(p, rq);
-}
-
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice
- *
- * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	int ret;
-
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-
-	/*preempt_disable();
-	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
-	ret = p->static_prio - MAX_RT_PRIO;
-	/*preempt_enable();*/
-
-	return ret;
-}
-
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-{
-	time_slice_expired(p, rq);
-}
-
-#ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p) {}
-#endif
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.33.0

