From 54606c5a70f628b3006bf0f9398b613fcfd7fe77 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 24 May 2021 21:46:42 +0000
Subject: [PATCH 159/204] sched/pds: Fix unexpected larger delta in
 task_sched_prio_normal()

---
 kernel/sched/alt_core.c | 23 ++++++++++++-----------
 kernel/sched/bmq.h      | 10 ++++++----
 kernel/sched/pds.h      | 39 +++++++++++++++++++++------------------
 3 files changed, 39 insertions(+), 33 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index c81a9fc6a140..21dc24e855eb 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1328,6 +1328,7 @@ static struct rq *move_queued_task(struct rq *rq, struct task_struct *p, int
 
 	raw_spin_lock(&rq->lock);
 	BUG_ON(task_cpu(p) != new_cpu);
+	sched_task_sanity_check(p, rq);
 	enqueue_task(p, rq, 0);
 	p->on_rq = TASK_ON_RQ_QUEUED;
 	check_preempt_curr(rq);
@@ -1656,7 +1657,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	return dest_cpu;
 }
 
-static inline int select_task_rq(struct task_struct *p, struct rq *rq)
+static inline int select_task_rq(struct task_struct *p)
 {
 	cpumask_t chk_mask, tmp;
 
@@ -1669,7 +1670,7 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 #endif
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
+			&sched_rq_watermark[task_sched_prio(p) + 1]))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -1823,7 +1824,7 @@ EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 #else /* CONFIG_SMP */
 
-static inline int select_task_rq(struct task_struct *p, struct rq *rq)
+static inline int select_task_rq(struct task_struct *p)
 {
 	return 0;
 }
@@ -2360,7 +2361,7 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 
 	sched_task_ttwu(p);
 
-	cpu = select_task_rq(p, this_rq());
+	cpu = select_task_rq(p);
 
 	if (cpu != task_cpu(p)) {
 		if (p->in_iowait) {
@@ -2662,7 +2663,7 @@ void wake_up_new_task(struct task_struct *p)
 
 	p->state = TASK_RUNNING;
 
-	rq = cpu_rq(select_task_rq(p, this_rq()));
+	rq = cpu_rq(select_task_rq(p));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
 	/*
@@ -3265,7 +3266,7 @@ void sched_exec(void)
 	if (rq != task_rq(p) || rq->nr_running < 2)
 		goto unlock;
 
-	dest_cpu = select_task_rq(p, task_rq(p));
+	dest_cpu = select_task_rq(p);
 	if (dest_cpu == smp_processor_id())
 		goto unlock;
 
@@ -3847,7 +3848,7 @@ inline void alt_sched_debug(void)
 {
 	int i;
 
-	for (i = 0; i < 3; i++)
+	for (i = 0; i < 6; i++)
 		printk(KERN_INFO "sched: %d\n", alt_debug[i]);
 }
 #endif
@@ -4562,7 +4563,7 @@ int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flag
 }
 EXPORT_SYMBOL(default_wake_function);
 
-static inline void check_task_changed(struct rq *rq, struct task_struct *p)
+static inline void check_task_changed(struct task_struct *p, struct rq *rq)
 {
 	/* Trigger resched if task sched_prio has been modified. */
 	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
@@ -4654,7 +4655,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 	trace_sched_pi_setprio(p, pi_task);
 	p->prio = prio;
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 out_unlock:
 	/* Avoid rq from going away on us: */
 	preempt_disable();
@@ -4698,7 +4699,7 @@ void set_user_nice(struct task_struct *p, long nice)
 
 	p->prio = effective_prio(p);
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 out_unlock:
 	__task_access_unlock(p, lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
@@ -5027,7 +5028,7 @@ static int __sched_setscheduler(struct task_struct *p,
 
 	__setscheduler(rq, p, attr, pi);
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 
 	/* Avoid rq from going away on us: */
 	preempt_disable();
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7858ac1185ce..eea8cb31ca1a 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -44,7 +44,7 @@ static inline int normal_prio(struct task_struct *p)
 	return p->static_prio + MAX_PRIORITY_ADJ;
 }
 
-static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+static inline int task_sched_prio(struct task_struct *p)
 {
 	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
 }
@@ -62,6 +62,8 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	}
 }
 
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
+
 static inline void sched_imp_init(void) {}
 
 inline int task_running_nice(struct task_struct *p)
@@ -110,13 +112,13 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->sq_idx = task_sched_prio(p, rq);				\
+	p->sq_idx = task_sched_prio(p);					\
 	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
 	set_bit(p->sq_idx, rq->queue.bitmap)
 
 #define __SCHED_REQUEUE_TASK(p, rq, func)				\
 {									\
-	int idx = task_sched_prio(p, rq);				\
+	int idx = task_sched_prio(p);					\
 \
 	list_del(&p->sq_node);						\
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
@@ -131,7 +133,7 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	return (task_sched_prio(p, rq) != p->sq_idx);
+	return (task_sched_prio(p) != p->sq_idx);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 62b5ab738876..7eac80b83fb3 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -21,23 +21,22 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	int delta;
+	int delta = (p->deadline >> 23) - rq->time_edge  - 1;
 
-	delta = rq->time_edge + 20 - (p->deadline >> 23);
-	if (delta < 0) {
-		delta = 0;
-		alt_debug[0]++;
+	if (unlikely(delta > 19)) {
+		pr_info("pds: task_sched_prio_normal delta %d, deadline %llu(%llu), time_edge %llu\n",
+			delta, p->deadline, p->deadline >> 23, rq->time_edge);
+		delta = 19;
 	}
-	delta = 19 - min(delta, 19);
 
-	return delta;
+	return (delta < 0)? 0:delta;
 }
 
 static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
+task_sched_prio(const struct task_struct *p)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+		return MAX_RT_PRIO + task_sched_prio_normal(p, task_rq(p));
 
 	return p->prio;
 }
@@ -92,7 +91,7 @@ static inline int normal_prio(struct task_struct *p)
 
 int task_running_nice(struct task_struct *p)
 {
-	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
+	return task_sched_prio(p) > DEFAULT_SCHED_PRIO;
 }
 
 static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
@@ -117,7 +116,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 	if (now == old)
 		return;
 
-	delta = min(20ULL, now - old);
+	delta = min_t(u64, 20, now - old);
 	INIT_LIST_HEAD(&head);
 
 	prio = MAX_RT_PRIO;
@@ -151,6 +150,12 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 		requeue_task(p, rq);
 }
 
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
+{
+	if (unlikely(p->deadline > rq->clock + 40 * SCHED_PRIO_SLOT))
+		p->deadline = rq->clock + 40 * SCHED_PRIO_SLOT;
+}
+
 static inline void sched_imp_init(void)
 {
 	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
@@ -212,11 +217,12 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 \
 	list_del(&p->sq_node);							\
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {						\
+	if (idx != p->sq_idx) {							\
 		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+			clear_bit(sched_idx2prio(p->sq_idx, rq),		\
+				  rq->queue.bitmap);				\
 		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);	\
 		func;								\
 	}									\
 }
@@ -249,10 +255,7 @@ int task_prio(const struct task_struct *p)
 	if (p->prio < MAX_RT_PRIO)
 		return (p->prio - MAX_RT_PRIO);
 
-	/*preempt_disable();
-	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
-	ret = p->static_prio - MAX_RT_PRIO;
-	/*preempt_enable();*/
+	ret = task_sched_prio(p) - MAX_RT_PRIO;
 
 	return ret;
 }
-- 
2.33.0

