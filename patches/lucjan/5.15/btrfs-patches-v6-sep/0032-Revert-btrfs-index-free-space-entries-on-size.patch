From be76df5883974e7800efde7b9e5eb54926870125 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Fri, 19 Nov 2021 21:30:28 +0100
Subject: [PATCH 32/40] Revert "btrfs: index free space entries on size"

This reverts commit d599d3b21dc57c2886666c9383778d2abef0ece0.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 fs/btrfs/free-space-cache.c       | 198 +++++++-----------------------
 fs/btrfs/free-space-cache.h       |   2 -
 fs/btrfs/tests/free-space-tests.c | 181 ---------------------------
 3 files changed, 44 insertions(+), 337 deletions(-)

diff --git a/fs/btrfs/free-space-cache.c b/fs/btrfs/free-space-cache.c
index 349bea1..6ca3a92 100644
--- a/fs/btrfs/free-space-cache.c
+++ b/fs/btrfs/free-space-cache.c
@@ -1576,68 +1576,6 @@ static int tree_insert_offset(struct rb_root *root, u64 offset,
 	return 0;
 }
 
-/*
- * This is a little subtle.  We *only* have ->max_extent_size set if we actually
- * searched through the bitmap and figured out the largest ->max_extent_size,
- * otherwise it's 0.  In the case that it's 0 we don't want to tell the
- * allocator the wrong thing, we want to use the actual real max_extent_size
- * we've found already if it's larger, or we want to use ->bytes.
- *
- * This matters because find_free_space() will skip entries who's ->bytes is
- * less than the required bytes.  So if we didn't search down this bitmap, we
- * may pick some previous entry that has a smaller ->max_extent_size than we
- * have.  For example, assume we have two entries, one that has
- * ->max_extent_size set to 4k and ->bytes set to 1M.  A second entry hasn't set
- * ->max_extent_size yet, has ->bytes set to 8k and it's contiguous.  We will
- *  call into find_free_space(), and return with max_extent_size == 4k, because
- *  that first bitmap entry had ->max_extent_size set, but the second one did
- *  not.  If instead we returned 8k we'd come in searching for 8k, and find the
- *  8k contiguous range.
- *
- *  Consider the other case, we have 2 8k chunks in that second entry and still
- *  don't have ->max_extent_size set.  We'll return 16k, and the next time the
- *  allocator comes in it'll fully search our second bitmap, and this time it'll
- *  get an uptodate value of 8k as the maximum chunk size.  Then we'll get the
- *  right allocation the next loop through.
- */
-static inline u64 get_max_extent_size(struct btrfs_free_space *entry)
-{
-	if (entry->bitmap && entry->max_extent_size)
-		return entry->max_extent_size;
-	return entry->bytes;
-}
-
-/*
- * This is indexed in reverse of what we generally do for rb-tree's, the largest
- * chunks are left most and the smallest are rightmost.  This is so that we can
- * take advantage of the cached property of the cached rb-tree and simply get
- * the largest free space chunk right away.
- */
-static void tree_insert_bytes(struct btrfs_free_space_ctl *ctl,
-			      struct btrfs_free_space *info)
-{
-	struct rb_root_cached *root = &ctl->free_space_bytes;
-	struct rb_node **p = &root->rb_root.rb_node;
-	struct rb_node *parent_node = NULL;
-	struct btrfs_free_space *tmp;
-	bool leftmost = true;
-
-	while (*p) {
-		parent_node = *p;
-		tmp = rb_entry(parent_node, struct btrfs_free_space,
-			       bytes_index);
-		if (get_max_extent_size(info) < get_max_extent_size(tmp)) {
-			p = &(*p)->rb_right;
-			leftmost = false;
-		} else {
-			p = &(*p)->rb_left;
-		}
-	}
-
-	rb_link_node(&info->bytes_index, parent_node, p);
-	rb_insert_color_cached(&info->bytes_index, root, leftmost);
-}
-
 /*
  * searches the tree for the given offset.
  *
@@ -1766,7 +1704,6 @@ __unlink_free_space(struct btrfs_free_space_ctl *ctl,
 		    struct btrfs_free_space *info)
 {
 	rb_erase(&info->offset_index, &ctl->free_space_offset);
-	rb_erase_cached(&info->bytes_index, &ctl->free_space_bytes);
 	ctl->free_extents--;
 
 	if (!info->bitmap && !btrfs_free_space_trimmed(info)) {
@@ -1793,8 +1730,6 @@ static int link_free_space(struct btrfs_free_space_ctl *ctl,
 	if (ret)
 		return ret;
 
-	tree_insert_bytes(ctl, info);
-
 	if (!info->bitmap && !btrfs_free_space_trimmed(info)) {
 		ctl->discardable_extents[BTRFS_STAT_CURR]++;
 		ctl->discardable_bytes[BTRFS_STAT_CURR] += info->bytes;
@@ -1805,22 +1740,6 @@ static int link_free_space(struct btrfs_free_space_ctl *ctl,
 	return ret;
 }
 
-static void relink_bitmap_entry(struct btrfs_free_space_ctl *ctl,
-				struct btrfs_free_space *info)
-{
-	ASSERT(info->bitmap);
-
-	/*
-	 * If our entry is empty it's because we're on a cluster and we don't
-	 * want to re-link it into our ctl bytes index.
-	 */
-	if (RB_EMPTY_NODE(&info->bytes_index))
-		return;
-
-	rb_erase_cached(&info->bytes_index, &ctl->free_space_bytes);
-	tree_insert_bytes(ctl, info);
-}
-
 static inline void __bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,
 				       struct btrfs_free_space *info,
 				       u64 offset, u64 bytes)
@@ -1839,8 +1758,6 @@ static inline void __bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,
 	if (info->max_extent_size > ctl->unit)
 		info->max_extent_size = 0;
 
-	relink_bitmap_entry(ctl, info);
-
 	if (start && test_bit(start - 1, info->bitmap))
 		extent_delta++;
 
@@ -1876,16 +1793,9 @@ static void bitmap_set_bits(struct btrfs_free_space_ctl *ctl,
 
 	bitmap_set(info->bitmap, start, count);
 
-	/*
-	 * We set some bytes, we have no idea what the max extent size is
-	 * anymore.
-	 */
-	info->max_extent_size = 0;
 	info->bytes += bytes;
 	ctl->free_space += bytes;
 
-	relink_bitmap_entry(ctl, info);
-
 	if (start && test_bit(start - 1, info->bitmap))
 		extent_delta--;
 
@@ -1953,14 +1863,44 @@ static int search_bitmap(struct btrfs_free_space_ctl *ctl,
 
 	*bytes = (u64)(max_bits) * ctl->unit;
 	bitmap_info->max_extent_size = *bytes;
-	relink_bitmap_entry(ctl, bitmap_info);
 	return -1;
 }
 
+/*
+ * This is a little subtle.  We *only* have ->max_extent_size set if we actually
+ * searched through the bitmap and figured out the largest ->max_extent_size,
+ * otherwise it's 0.  In the case that it's 0 we don't want to tell the
+ * allocator the wrong thing, we want to use the actual real max_extent_size
+ * we've found already if it's larger, or we want to use ->bytes.
+ *
+ * This matters because find_free_space() will skip entries who's ->bytes is
+ * less than the required bytes.  So if we didn't search down this bitmap, we
+ * may pick some previous entry that has a smaller ->max_extent_size than we
+ * have.  For example, assume we have two entries, one that has
+ * ->max_extent_size set to 4k and ->bytes set to 1M.  A second entry hasn't set
+ * ->max_extent_size yet, has ->bytes set to 8k and it's contiguous.  We will
+ *  call into find_free_space(), and return with max_extent_size == 4k, because
+ *  that first bitmap entry had ->max_extent_size set, but the second one did
+ *  not.  If instead we returned 8k we'd come in searching for 8k, and find the
+ *  8k contiguous range.
+ *
+ *  Consider the other case, we have 2 8k chunks in that second entry and still
+ *  don't have ->max_extent_size set.  We'll return 16k, and the next time the
+ *  allocator comes in it'll fully search our second bitmap, and this time it'll
+ *  get an uptodate value of 8k as the maximum chunk size.  Then we'll get the
+ *  right allocation the next loop through.
+ */
+static inline u64 get_max_extent_size(struct btrfs_free_space *entry)
+{
+	if (entry->bitmap && entry->max_extent_size)
+		return entry->max_extent_size;
+	return entry->bytes;
+}
+
 /* Cache the size of the max extent in bytes */
 static struct btrfs_free_space *
 find_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,
-		unsigned long align, u64 *max_extent_size, bool use_bytes_index)
+		unsigned long align, u64 *max_extent_size)
 {
 	struct btrfs_free_space *entry;
 	struct rb_node *node;
@@ -1970,38 +1910,16 @@ find_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,
 
 	if (!ctl->free_space_offset.rb_node)
 		goto out;
-again:
-	if (use_bytes_index) {
-		node = rb_first_cached(&ctl->free_space_bytes);
-	} else {
-		entry = tree_search_offset(ctl, offset_to_bitmap(ctl, *offset),
-					   0, 1);
-		if (!entry)
-			goto out;
-		node = &entry->offset_index;
-	}
 
-	for (; node; node = rb_next(node)) {
-		if (use_bytes_index)
-			entry = rb_entry(node, struct btrfs_free_space,
-					 bytes_index);
-		else
-			entry = rb_entry(node, struct btrfs_free_space,
-					 offset_index);
+	entry = tree_search_offset(ctl, offset_to_bitmap(ctl, *offset), 0, 1);
+	if (!entry)
+		goto out;
 
-		/*
-		 * If we are using the bytes index then all subsequent entries
-		 * in this tree are going to be < bytes, so simply set the max
-		 * extent size and exit the loop.
-		 *
-		 * If we're using the offset index then we need to keep going
-		 * through the rest of the tree.
-		 */
+	for (node = &entry->offset_index; node; node = rb_next(node)) {
+		entry = rb_entry(node, struct btrfs_free_space, offset_index);
 		if (entry->bytes < *bytes) {
 			*max_extent_size = max(get_max_extent_size(entry),
 					       *max_extent_size);
-			if (use_bytes_index)
-				break;
 			continue;
 		}
 
@@ -2018,13 +1936,6 @@ find_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,
 			tmp = entry->offset;
 		}
 
-		/*
-		 * We don't break here if we're using the bytes index because we
-		 * may have another entry that has the correct alignment that is
-		 * the right size, so we don't want to miss that possibility.
-		 * At worst this adds another loop through the logic, but if we
-		 * broke here we could prematurely ENOSPC.
-		 */
 		if (entry->bytes < *bytes + align_off) {
 			*max_extent_size = max(get_max_extent_size(entry),
 					       *max_extent_size);
@@ -2032,7 +1943,6 @@ find_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,
 		}
 
 		if (entry->bitmap) {
-			struct rb_node *old_next = rb_next(node);
 			u64 size = *bytes;
 
 			ret = search_bitmap(ctl, entry, &tmp, &size, true);
@@ -2045,15 +1955,6 @@ find_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,
 					max(get_max_extent_size(entry),
 					    *max_extent_size);
 			}
-
-			/*
-			 * The bitmap may have gotten re-arranged in the space
-			 * index here because the max_extent_size may have been
-			 * updated.  Start from the beginning again if this
-			 * happened.
-			 */
-			if (use_bytes_index && old_next != rb_next(node))
-				goto again;
 			continue;
 		}
 
@@ -2202,6 +2103,12 @@ static u64 add_bytes_to_bitmap(struct btrfs_free_space_ctl *ctl,
 
 	bitmap_set_bits(ctl, info, offset, bytes_to_set);
 
+	/*
+	 * We set some bytes, we have no idea what the max extent size is
+	 * anymore.
+	 */
+	info->max_extent_size = 0;
+
 	return bytes_to_set;
 
 }
@@ -2599,7 +2506,6 @@ int __btrfs_add_free_space(struct btrfs_fs_info *fs_info,
 	info->bytes = bytes;
 	info->trim_state = trim_state;
 	RB_CLEAR_NODE(&info->offset_index);
-	RB_CLEAR_NODE(&info->bytes_index);
 
 	spin_lock(&ctl->tree_lock);
 
@@ -2903,7 +2809,6 @@ void btrfs_init_free_space_ctl(struct btrfs_block_group *block_group,
 	ctl->start = block_group->start;
 	ctl->private = block_group;
 	ctl->op = &free_space_op;
-	ctl->free_space_bytes = RB_ROOT_CACHED;
 	INIT_LIST_HEAD(&ctl->trimming_ranges);
 	mutex_init(&ctl->cache_writeout_mutex);
 
@@ -2969,7 +2874,6 @@ static void __btrfs_return_cluster_to_free_space(
 		}
 		tree_insert_offset(&ctl->free_space_offset,
 				   entry->offset, &entry->offset_index, bitmap);
-		tree_insert_bytes(ctl, entry);
 	}
 	cluster->root = RB_ROOT;
 	spin_unlock(&cluster->lock);
@@ -3071,14 +2975,12 @@ u64 btrfs_find_space_for_alloc(struct btrfs_block_group *block_group,
 	u64 align_gap = 0;
 	u64 align_gap_len = 0;
 	enum btrfs_trim_state align_gap_trim_state = BTRFS_TRIM_STATE_UNTRIMMED;
-	bool use_bytes_index = (offset == block_group->start);
 
 	ASSERT(!btrfs_is_zoned(block_group->fs_info));
 
 	spin_lock(&ctl->tree_lock);
 	entry = find_free_space(ctl, &offset, &bytes_search,
-				block_group->full_stripe_len, max_extent_size,
-				use_bytes_index);
+				block_group->full_stripe_len, max_extent_size);
 	if (!entry)
 		goto out;
 
@@ -3362,17 +3264,6 @@ static int btrfs_bitmap_cluster(struct btrfs_block_group *block_group,
 
 	cluster->window_start = start * ctl->unit + entry->offset;
 	rb_erase(&entry->offset_index, &ctl->free_space_offset);
-	rb_erase_cached(&entry->bytes_index, &ctl->free_space_bytes);
-
-	/*
-	 * We need to know if we're currently on the normal space index when we
-	 * manipulate the bitmap so that we know we need to remove and re-insert
-	 * it into the space_index tree.  Clear the bytes_index node here so the
-	 * bitmap manipulation helpers know not to mess with the space_index
-	 * until this bitmap entry is added back into the normal cache.
-	 */
-	RB_CLEAR_NODE(&entry->bytes_index);
-
 	ret = tree_insert_offset(&cluster->root, entry->offset,
 				 &entry->offset_index, 1);
 	ASSERT(!ret); /* -EEXIST; Logic error */
@@ -3463,7 +3354,6 @@ setup_cluster_no_bitmap(struct btrfs_block_group *block_group,
 			continue;
 
 		rb_erase(&entry->offset_index, &ctl->free_space_offset);
-		rb_erase_cached(&entry->bytes_index, &ctl->free_space_bytes);
 		ret = tree_insert_offset(&cluster->root, entry->offset,
 					 &entry->offset_index, 0);
 		total_size += entry->bytes;
diff --git a/fs/btrfs/free-space-cache.h b/fs/btrfs/free-space-cache.h
index dd982d2..1f23088 100644
--- a/fs/btrfs/free-space-cache.h
+++ b/fs/btrfs/free-space-cache.h
@@ -22,7 +22,6 @@ enum btrfs_trim_state {
 
 struct btrfs_free_space {
 	struct rb_node offset_index;
-	struct rb_node bytes_index;
 	u64 offset;
 	u64 bytes;
 	u64 max_extent_size;
@@ -46,7 +45,6 @@ static inline bool btrfs_free_space_trimming_bitmap(
 struct btrfs_free_space_ctl {
 	spinlock_t tree_lock;
 	struct rb_root free_space_offset;
-	struct rb_root_cached free_space_bytes;
 	u64 free_space;
 	int extents_thresh;
 	int free_extents;
diff --git a/fs/btrfs/tests/free-space-tests.c b/fs/btrfs/tests/free-space-tests.c
index 6f922ce..8f05c1e 100644
--- a/fs/btrfs/tests/free-space-tests.c
+++ b/fs/btrfs/tests/free-space-tests.c
@@ -824,184 +824,6 @@ test_steal_space_from_bitmap_to_extent(struct btrfs_block_group *cache,
 	return 0;
 }
 
-static bool bytes_index_use_bitmap(struct btrfs_free_space_ctl *ctl,
-				   struct btrfs_free_space *info)
-{
-	return true;
-}
-
-static int test_bytes_index(struct btrfs_block_group *cache, u32 sectorsize)
-{
-	const struct btrfs_free_space_op test_free_space_ops = {
-		.use_bitmap = bytes_index_use_bitmap,
-	};
-	const struct btrfs_free_space_op *orig_free_space_ops;
-	struct btrfs_free_space_ctl *ctl = cache->free_space_ctl;
-	struct btrfs_free_space *entry;
-	struct rb_node *n;
-	u64 offset, max_extent_size, bytes;
-	int ret, i;
-
-	test_msg("running bytes index tests");
-
-	/* First just validate that it does everything in order. */
-	offset = 0;
-	for (i = 0; i < 10; i++) {
-		bytes = (i + 1) * SZ_1M;
-		ret = test_add_free_space_entry(cache, offset, bytes, 0);
-		if (ret) {
-			test_err("couldn't add extent entry %d\n", ret);
-			return ret;
-		}
-		offset += bytes + sectorsize;
-	}
-
-	for (n = rb_first_cached(&ctl->free_space_bytes), i = 9; n;
-	     n = rb_next(n), i--) {
-		entry = rb_entry(n, struct btrfs_free_space, bytes_index);
-		bytes = (i + 1) * SZ_1M;
-		if (entry->bytes != bytes) {
-			test_err("invalid bytes index order, found %llu expected %llu",
-				 entry->bytes, bytes);
-			return -EINVAL;
-		}
-	}
-
-	/* Now validate bitmaps do the correct thing. */
-	__btrfs_remove_free_space_cache(cache->free_space_ctl);
-	for (i = 0; i < 2; i++) {
-		offset = i * BITS_PER_BITMAP * sectorsize;
-		bytes = (i + 1) * SZ_1M;
-		ret = test_add_free_space_entry(cache, offset, bytes, 1);
-		if (ret) {
-			test_err("couldn't add bitmap entry");
-			return ret;
-		}
-	}
-
-	for (n = rb_first_cached(&ctl->free_space_bytes), i = 1; n;
-	     n = rb_next(n), i--) {
-		entry = rb_entry(n, struct btrfs_free_space, bytes_index);
-		bytes = (i + 1) * SZ_1M;
-		if (entry->bytes != bytes) {
-			test_err("invalid bytes index order, found %llu expected %llu",
-				 entry->bytes, bytes);
-			return -EINVAL;
-		}
-	}
-
-	/* Now validate bitmaps with different ->max_extent_size. */
-	__btrfs_remove_free_space_cache(cache->free_space_ctl);
-	orig_free_space_ops = cache->free_space_ctl->op;
-	cache->free_space_ctl->op = &test_free_space_ops;
-
-	ret = test_add_free_space_entry(cache, 0, sectorsize, 1);
-	if (ret) {
-		test_err("couldn't add bitmap entry");
-		return ret;
-	}
-
-	offset = BITS_PER_BITMAP * sectorsize;
-	ret = test_add_free_space_entry(cache, offset, sectorsize, 1);
-	if (ret) {
-		test_err("couldn't add bitmap_entry");
-		return ret;
-	}
-
-	/*
-	 * Now set a bunch of sectorsize extents in the first entry so it's
-	 * ->bytes is large.
-	 */
-	for (i = 2; i < 20; i += 2) {
-		offset = sectorsize * i;
-		ret = btrfs_add_free_space(cache, offset, sectorsize);
-		if (ret) {
-			test_err("error populating sparse bitmap %d", ret);
-			return ret;
-		}
-	}
-
-	/*
-	 * Now set a contiguous extent in the second bitmap so its
-	 * ->max_extent_size is larger than the first bitmaps.
-	 */
-	offset = (BITS_PER_BITMAP * sectorsize) + sectorsize;
-	ret = btrfs_add_free_space(cache, offset, sectorsize);
-	if (ret) {
-		test_err("error adding contiguous extent %d", ret);
-		return ret;
-	}
-
-	/*
-	 * Since we don't set ->max_extent_size unless we search everything
-	 * should be indexed on bytes.
-	 */
-	entry = rb_entry(rb_first_cached(&ctl->free_space_bytes),
-			 struct btrfs_free_space, bytes_index);
-	if (entry->bytes != (10 * sectorsize)) {
-		test_err("error, wrong entry in the first slot in bytes_index");
-		return -EINVAL;
-	}
-
-	max_extent_size = 0;
-	offset = btrfs_find_space_for_alloc(cache, cache->start, sectorsize * 3,
-					    0, &max_extent_size);
-	if (offset != 0) {
-		test_err("found space to alloc even though we don't have enough space");
-		return -EINVAL;
-	}
-
-	if (max_extent_size != (2 * sectorsize)) {
-		test_err("got the wrong max_extent size %llu expected %llu",
-			 max_extent_size, (unsigned long long)(2 * sectorsize));
-		return -EINVAL;
-	}
-
-	/*
-	 * The search should have re-arranged the bytes index to use the
-	 * ->max_extent_size, validate it's now what we expect it to be.
-	 */
-	entry = rb_entry(rb_first_cached(&ctl->free_space_bytes),
-			 struct btrfs_free_space, bytes_index);
-	if (entry->bytes != (2 * sectorsize)) {
-		test_err("error, the bytes index wasn't recalculated properly");
-		return -EINVAL;
-	}
-
-	/* Add another sectorsize to re-arrange the tree back to ->bytes. */
-	offset = (BITS_PER_BITMAP * sectorsize) - sectorsize;
-	ret = btrfs_add_free_space(cache, offset, sectorsize);
-	if (ret) {
-		test_err("error adding extent to the sparse entry %d", ret);
-		return ret;
-	}
-
-	entry = rb_entry(rb_first_cached(&ctl->free_space_bytes),
-			 struct btrfs_free_space, bytes_index);
-	if (entry->bytes != (11 * sectorsize)) {
-		test_err("error, wrong entry in the first slot in bytes_index");
-		return -EINVAL;
-	}
-
-	/*
-	 * Now make sure we find our correct entry after searching that will
-	 * result in a re-arranging of the tree.
-	 */
-	max_extent_size = 0;
-	offset = btrfs_find_space_for_alloc(cache, cache->start, sectorsize * 2,
-					    0, &max_extent_size);
-	if (offset != (BITS_PER_BITMAP * sectorsize)) {
-		test_err("error, found %llu instead of %llu for our alloc",
-			 offset,
-			 (unsigned long long)(BITS_PER_BITMAP * sectorsize));
-		return -EINVAL;
-	}
-
-	cache->free_space_ctl->op = orig_free_space_ops;
-	__btrfs_remove_free_space_cache(cache->free_space_ctl);
-	return 0;
-}
-
 int btrfs_test_free_space_cache(u32 sectorsize, u32 nodesize)
 {
 	struct btrfs_fs_info *fs_info;
@@ -1049,9 +871,6 @@ int btrfs_test_free_space_cache(u32 sectorsize, u32 nodesize)
 		goto out;
 
 	ret = test_steal_space_from_bitmap_to_extent(cache, sectorsize);
-	if (ret)
-		goto out;
-	ret = test_bytes_index(cache, sectorsize);
 out:
 	btrfs_free_dummy_block_group(cache);
 	btrfs_free_dummy_root(root);
-- 
2.34.1.75.gabe6bb3905

