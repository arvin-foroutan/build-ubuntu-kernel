From ed3fba3c5f747c9db3e6175e866e63ffdedd9e26 Mon Sep 17 00:00:00 2001
From: Steven Barrett <steven@liquorix.net>
Date: Thu, 25 Nov 2021 11:12:55 -0600
Subject: [PATCH 1/3] sched/alt: [Sync] 42dc938a sched/core: Mitigate race
 cpus_share_cache()/update_top_cache_domain()

---
 kernel/sched/alt_core.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 8b0ddbdd24e4..db05c9c49446 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -2449,6 +2449,9 @@ void wake_up_if_idle(int cpu)
 
 bool cpus_share_cache(int this_cpu, int that_cpu)
 {
+	if (this_cpu == that_cpu)
+		return true;
+
 	return per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu);
 }
 #else /* !CONFIG_SMP */
-- 
2.34.1.75.gabe6bb3905


From b5da1de43d87642b96cd1d22a85ec089ced5044c Mon Sep 17 00:00:00 2001
From: Steven Barrett <steven@liquorix.net>
Date: Thu, 25 Nov 2021 12:54:22 -0600
Subject: [PATCH 2/3] sched/alt: [Sync] b027789e Prevent dead task groups from
 regaining cfs_rq's

---
 kernel/sched/alt_core.c | 30 ++++++++++++++++++++++--------
 1 file changed, 22 insertions(+), 8 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index db05c9c49446..743498c877da 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -7452,6 +7452,20 @@ static void sched_free_group(struct task_group *tg)
 	kmem_cache_free(task_group_cache, tg);
 }
 
+static void sched_free_group_rcu(struct rcu_head *rcu)
+{
+	sched_free_group(container_of(rcu, struct task_group, rcu));
+}
+
+static void sched_unregister_group(struct task_group *tg)
+{
+	/*
+	 * We have to wait for yet another RCU grace period to expire, as
+	 * print_cfs_stats() might run concurrently.
+	 */
+	call_rcu(&tg->rcu, sched_free_group_rcu);
+}
+
 /* allocate runqueue etc for a new task group */
 struct task_group *sched_create_group(struct task_group *parent)
 {
@@ -7469,19 +7483,19 @@ void sched_online_group(struct task_group *tg, struct task_group *parent)
 }
 
 /* rcu callback to free various structures associated with a task group */
-static void sched_free_group_rcu(struct rcu_head *rhp)
+static void sched_unregister_group_rcu(struct rcu_head *rhp)
 {
-	/* Now it should be safe to free those cfs_rqs */
-	sched_free_group(container_of(rhp, struct task_group, rcu));
+	/* Now it should be safe to free those cfs_rqs: */
+	sched_unregister_group(container_of(rhp, struct task_group, rcu));
 }
 
 void sched_destroy_group(struct task_group *tg)
 {
-	/* Wait for possible concurrent references to cfs_rqs complete */
-	call_rcu(&tg->rcu, sched_free_group_rcu);
+	/* Wait for possible concurrent references to cfs_rqs complete: */
+	call_rcu(&tg->rcu, sched_unregister_group_rcu);
 }
 
-void sched_offline_group(struct task_group *tg)
+void sched_release_group(struct task_group *tg)
 {
 }
 
@@ -7522,7 +7536,7 @@ static void cpu_cgroup_css_released(struct cgroup_subsys_state *css)
 {
 	struct task_group *tg = css_tg(css);
 
-	sched_offline_group(tg);
+	sched_release_group(tg);
 }
 
 static void cpu_cgroup_css_free(struct cgroup_subsys_state *css)
@@ -7532,7 +7546,7 @@ static void cpu_cgroup_css_free(struct cgroup_subsys_state *css)
 	/*
 	 * Relies on the RCU grace period between css_released() and this.
 	 */
-	sched_free_group(tg);
+	sched_unregister_group(tg);
 }
 
 static void cpu_cgroup_fork(struct task_struct *task)
-- 
2.34.1.75.gabe6bb3905


From 8c76d77010bc5cc725ad26252a49a174e204660a Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Sun, 21 Nov 2021 23:58:50 +0100
Subject: [PATCH 3/3] sched/alt: Optimize loops in update_sched_rq_watermark.

With the old code, gcc misses an optimization opportunity and compiles
the loops to five instructions each:

    0x0000000000000ed3 <+83>:	lock bts %rdi,(%rax)
    0x0000000000000ed8 <+88>:	dec    %rdx
    0x0000000000000edb <+91>:	add    $0x400,%rax
    0x0000000000000ee1 <+97>:	cmp    %rdx,%rsi
    0x0000000000000ee4 <+100>:	jne    0xed3 <update_sched_rq_watermark+83>
    ...
    0x0000000000000f13 <+147>:	lock btr %rdi,(%rax)
    0x0000000000000f18 <+152>:	dec    %rdx
    0x0000000000000f1b <+155>:	add    $0x400,%rax
    0x0000000000000f21 <+161>:	cmp    %rcx,%rdx
    0x0000000000000f24 <+164>:	jne    0xf13 <update_sched_rq_watermark+147>

With this change, the loops get optimized to four instructions each:

    0x0000000000000ed7 <+87>:	lock bts %rsi,(%rdx)
    0x0000000000000edc <+92>:	add    $0x400,%rdx
    0x0000000000000ee3 <+99>:	dec    %rcx
    0x0000000000000ee6 <+102>:	jne    0xed7 <update_sched_rq_watermark+87>
    ...
    0x0000000000000f1a <+154>:	lock btr %rsi,(%rax)
    0x0000000000000f1f <+159>:	add    $0x400,%rax
    0x0000000000000f25 <+165>:	dec    %rdx
    0x0000000000000f28 <+168>:	jne    0xf1a <update_sched_rq_watermark+154>

Signed-off-by: Torge Matthies <openglfreak@googlemail.com>
---
 kernel/sched/alt_core.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 743498c877da..668c6a87e1d2 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -185,8 +185,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = last_wm; i > watermark; i--)
-			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+		for (i = last_wm - watermark; i > 0; i--)
+			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + watermark));
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
 		    IDLE_TASK_SCHED_PRIO == last_wm)
@@ -196,8 +196,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = watermark; i > last_wm; i--)
-		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+	for (i = watermark - last_wm; i > 0; i--)
+		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + last_wm));
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) &&
 	    IDLE_TASK_SCHED_PRIO == watermark) {
-- 
2.34.1.75.gabe6bb3905

