From bed17e393e6e234b7fca08bdcf8813cd788150c4 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 27 Jun 2021 14:45:03 +0000
Subject: [PATCH 1/7] sched/alt: Reverse sched_rq_watermark order

---
 kernel/sched/alt_core.c | 21 +++++++++++----------
 1 file changed, 11 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b65b12c6014f..ffe95d0b5856 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -189,9 +189,10 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = watermark + 1; i <= last_wm; i++)
-			cpumask_andnot(&sched_rq_watermark[i],
-				       &sched_rq_watermark[i], cpumask_of(cpu));
+		for (i = last_wm; i > watermark; i--)
+			cpumask_andnot(&sched_rq_watermark[SCHED_BITS - 1 - i],
+				       &sched_rq_watermark[SCHED_BITS - 1 - i],
+				       cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
 		    IDLE_WM == last_wm)
@@ -201,13 +202,13 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = last_wm + 1; i <= watermark; i++)
-		cpumask_set_cpu(cpu, &sched_rq_watermark[i]);
+	for (i = watermark; i > last_wm; i--)
+		cpumask_set_cpu(cpu, &sched_rq_watermark[SCHED_BITS - 1 - i]);
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) && IDLE_WM == watermark) {
 		cpumask_t tmp;
 
-		cpumask_and(&tmp, cpu_smt_mask(cpu), &sched_rq_watermark[IDLE_WM]);
+		cpumask_and(&tmp, cpu_smt_mask(cpu), &sched_rq_watermark[0]);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
 			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
 				   &sched_sg_idle_mask);
@@ -1736,9 +1737,9 @@ static inline int select_task_rq(struct task_struct *p)
 #ifdef CONFIG_SCHED_SMT
 	    cpumask_and(&tmp, &chk_mask, &sched_sg_idle_mask) ||
 #endif
-	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
+	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[0]) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[task_sched_prio(p) + 1]))
+			&sched_rq_watermark[SCHED_BITS - task_sched_prio(p)]))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -3592,7 +3593,7 @@ static inline void sg_balance_check(struct rq *rq)
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
 	    cpumask_andnot(&chk, cpu_online_mask, &sched_rq_pending_mask) &&
-	    cpumask_andnot(&chk, &chk, &sched_rq_watermark[IDLE_WM])) {
+	    cpumask_andnot(&chk, &chk, &sched_rq_watermark[0])) {
 		int i, tried = 0;
 
 		for_each_cpu_wrap(i, &chk, cpu) {
@@ -3905,7 +3906,7 @@ void alt_sched_debug(void)
 {
 	printk(KERN_INFO "sched: pending: 0x%04lx, idle: 0x%04lx, sg_idle: 0x%04lx\n",
 	       sched_rq_pending_mask.bits[0],
-	       sched_rq_watermark[IDLE_WM].bits[0],
+	       sched_rq_watermark[0].bits[0],
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
-- 
2.32.0.452.g940fe202ad


From ee294e7b2924cca170133a4a2c51e94cc4a59fb5 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 28 Jun 2021 12:52:23 +0000
Subject: [PATCH 2/7] sched/alt: Use atomic operation in
 update_sched_rq_watermark() and code clean-up

---
 kernel/sched/alt_core.c | 25 +++++++++++--------------
 1 file changed, 11 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index ffe95d0b5856..472d73646b67 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -146,8 +146,6 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 # define finish_arch_post_lock_switch()	do { } while (0)
 #endif
 
-#define IDLE_WM	(IDLE_TASK_SCHED_PRIO)
-
 #ifdef CONFIG_SCHED_SMT
 static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
@@ -190,12 +188,10 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
 		for (i = last_wm; i > watermark; i--)
-			cpumask_andnot(&sched_rq_watermark[SCHED_BITS - 1 - i],
-				       &sched_rq_watermark[SCHED_BITS - 1 - i],
-				       cpumask_of(cpu));
+			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
-		    IDLE_WM == last_wm)
+		    IDLE_TASK_SCHED_PRIO == last_wm)
 			cpumask_andnot(&sched_sg_idle_mask,
 				       &sched_sg_idle_mask, cpu_smt_mask(cpu));
 #endif
@@ -203,12 +199,13 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	}
 	/* last_wm < watermark */
 	for (i = watermark; i > last_wm; i--)
-		cpumask_set_cpu(cpu, &sched_rq_watermark[SCHED_BITS - 1 - i]);
+		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
 #ifdef CONFIG_SCHED_SMT
-	if (static_branch_likely(&sched_smt_present) && IDLE_WM == watermark) {
+	if (static_branch_likely(&sched_smt_present) &&
+	    IDLE_TASK_SCHED_PRIO == watermark) {
 		cpumask_t tmp;
 
-		cpumask_and(&tmp, cpu_smt_mask(cpu), &sched_rq_watermark[0]);
+		cpumask_and(&tmp, cpu_smt_mask(cpu), sched_rq_watermark);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
 			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
 				   &sched_sg_idle_mask);
@@ -1737,9 +1734,9 @@ static inline int select_task_rq(struct task_struct *p)
 #ifdef CONFIG_SCHED_SMT
 	    cpumask_and(&tmp, &chk_mask, &sched_sg_idle_mask) ||
 #endif
-	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[0]) ||
+	    cpumask_and(&tmp, &chk_mask, sched_rq_watermark) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[SCHED_BITS - task_sched_prio(p)]))
+			sched_rq_watermark + SCHED_BITS - task_sched_prio(p)))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -3593,7 +3590,7 @@ static inline void sg_balance_check(struct rq *rq)
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
 	    cpumask_andnot(&chk, cpu_online_mask, &sched_rq_pending_mask) &&
-	    cpumask_andnot(&chk, &chk, &sched_rq_watermark[0])) {
+	    cpumask_andnot(&chk, &chk, sched_rq_watermark)) {
 		int i, tried = 0;
 
 		for_each_cpu_wrap(i, &chk, cpu) {
@@ -6773,7 +6770,7 @@ void __init sched_init(void)
 
 #ifdef CONFIG_SMP
 	for (i = 0; i < SCHED_BITS; i++)
-		cpumask_copy(&sched_rq_watermark[i], cpu_present_mask);
+		cpumask_copy(sched_rq_watermark + i, cpu_present_mask);
 #endif
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -6787,7 +6784,7 @@ void __init sched_init(void)
 		rq = cpu_rq(i);
 
 		sched_queue_init(&rq->queue);
-		rq->watermark = IDLE_WM;
+		rq->watermark = IDLE_TASK_SCHED_PRIO;
 		rq->skip = NULL;
 
 		raw_spin_lock_init(&rq->lock);
-- 
2.32.0.452.g940fe202ad


From e6852c977be498346a12e372a91c19bf7bae7386 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 7 Jul 2021 10:38:54 +0000
Subject: [PATCH 3/7] sched/alt: inline some BMQ/PDS interfaces

---
 kernel/sched/bmq.h |  6 +++---
 kernel/sched/pds.h | 12 +++++-------
 2 files changed, 8 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7635c00dde7f..be3ee4a553ca 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -89,20 +89,20 @@ static void sched_task_fork(struct task_struct *p, struct rq *rq)
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
 }
 
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+static inline void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = MAX_PRIORITY_ADJ;
 }
 
 #ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p)
+static inline void sched_task_ttwu(struct task_struct *p)
 {
 	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
 		boost_task(p);
 }
 #endif
 
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
+static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
 	if (rq_switch_time(rq) < boost_threshold(p))
 		boost_task(p);
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 06d88e72b543..0f1f0d708b77 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -18,11 +18,9 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
 	s64 delta = p->deadline - rq->time_edge + NORMAL_PRIO_NUM - NICE_WIDTH;
 
-	if (unlikely(delta > NORMAL_PRIO_NUM - 1)) {
-		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu, time_edge %llu\n",
-			delta, p->deadline, rq->time_edge);
+	if (WARN_ONCE(delta > NORMAL_PRIO_NUM - 1,
+		      "pds: task_sched_prio_normal() delta %lld\n", delta))
 		return NORMAL_PRIO_NUM - 1;
-	}
 
 	return (delta < 0) ? 0 : delta;
 }
@@ -118,12 +116,12 @@ static void sched_task_fork(struct task_struct *p, struct rq *rq)
 	sched_renew_deadline(p, rq);
 }
 
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+static inline void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	time_slice_expired(p, rq);
 }
 
 #ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p) {}
+static inline void sched_task_ttwu(struct task_struct *p) {}
 #endif
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
+static inline void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.32.0.452.g940fe202ad


From cb4db81a21ea5ddf4dd457207a9b24190bb504f8 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 10 Jul 2021 15:28:11 +0000
Subject: [PATCH 4/7] sched/alt: Remove over design in best_mask_cpu()

---
 kernel/sched/alt_core.c  | 30 +++++++--------------
 kernel/sched/alt_sched.h | 58 ++++------------------------------------
 2 files changed, 15 insertions(+), 73 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 472d73646b67..38f2ef489b37 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -207,8 +207,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 
 		cpumask_and(&tmp, cpu_smt_mask(cpu), sched_rq_watermark);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
-			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
-				   &sched_sg_idle_mask);
+			cpumask_or(&sched_sg_idle_mask,
+				   &sched_sg_idle_mask, cpu_smt_mask(cpu));
 	}
 #endif
 }
@@ -3528,8 +3528,7 @@ static inline int active_load_balance_cpu_stop(void *data)
 	    cpumask_and(&tmp, p->cpus_ptr, &sched_sg_idle_mask) &&
 	    !is_migration_disabled(p)) {
 		int cpu = cpu_of(rq);
-		int dcpu = __best_mask_cpu(cpu, &tmp,
-					   per_cpu(sched_cpu_llc_mask, cpu));
+		int dcpu = __best_mask_cpu(&tmp, per_cpu(sched_cpu_llc_mask, cpu));
 		rq = move_queued_task(rq, p, dcpu);
 	}
 
@@ -3573,34 +3572,25 @@ static inline int sg_balance_trigger(const int cpu)
 static inline void sg_balance_check(struct rq *rq)
 {
 	cpumask_t chk;
-	int cpu;
-
-	/* exit when no sg in idle */
-	if (cpumask_empty(&sched_sg_idle_mask))
-		return;
+	int cpu = cpu_of(rq);
 
 	/* exit when cpu is offline */
 	if (unlikely(!rq->online))
 		return;
 
-	cpu = cpu_of(rq);
 	/*
 	 * Only cpu in slibing idle group will do the checking and then
 	 * find potential cpus which can migrate the current running task
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
-	    cpumask_andnot(&chk, cpu_online_mask, &sched_rq_pending_mask) &&
-	    cpumask_andnot(&chk, &chk, sched_rq_watermark)) {
-		int i, tried = 0;
+	    cpumask_andnot(&chk, cpu_online_mask, sched_rq_watermark) &&
+	    cpumask_andnot(&chk, &chk, &sched_rq_pending_mask)) {
+		int i;
 
 		for_each_cpu_wrap(i, &chk, cpu) {
-			if (cpumask_subset(cpu_smt_mask(i), &chk)) {
-				if (sg_balance_trigger(i))
-					return;
-				if (tried)
-					return;
-				tried++;
-			}
+			if (cpumask_subset(cpu_smt_mask(i), &chk) &&
+			    sg_balance_trigger(i))
+				return;
 		}
 	}
 }
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index f9f79422bf0e..7a48809550bf 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -302,68 +302,20 @@ enum {
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
-static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
-				  const cpumask_t *mask)
+static inline int
+__best_mask_cpu(const cpumask_t *cpumask, const cpumask_t *mask)
 {
-#if NR_CPUS <= 64
-	unsigned long t;
+	int cpu;
 
-	while ((t = cpumask->bits[0] & mask->bits[0]) == 0UL)
-		mask++;
-
-	return __ffs(t);
-#else
 	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
 		mask++;
+
 	return cpu;
-#endif
 }
 
 static inline int best_mask_cpu(int cpu, const cpumask_t *mask)
 {
-#if NR_CPUS <= 64
-	unsigned long llc_match;
-	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
-
-	if ((llc_match = mask->bits[0] & chk->bits[0])) {
-		unsigned long match;
-
-		chk = per_cpu(sched_cpu_topo_masks, cpu);
-		if (mask->bits[0] & chk->bits[0])
-			return cpu;
-
-#ifdef CONFIG_SCHED_SMT
-		chk++;
-		if ((match = mask->bits[0] & chk->bits[0]))
-			return __ffs(match);
-#endif
-
-		return __ffs(llc_match);
-	}
-
-	return __best_mask_cpu(cpu, mask, chk + 1);
-#else
-	cpumask_t llc_match;
-	cpumask_t *chk = per_cpu(sched_cpu_llc_mask, cpu);
-
-	if (cpumask_and(&llc_match, mask, chk)) {
-		cpumask_t tmp;
-
-		chk = per_cpu(sched_cpu_topo_masks, cpu);
-		if (cpumask_test_cpu(cpu, mask))
-			return cpu;
-
-#ifdef CONFIG_SCHED_SMT
-		chk++;
-		if (cpumask_and(&tmp, mask, chk))
-			return cpumask_any(&tmp);
-#endif
-
-		return cpumask_any(&llc_match);
-	}
-
-	return __best_mask_cpu(cpu, mask, chk + 1);
-#endif
+	return __best_mask_cpu(mask, per_cpu(sched_cpu_topo_masks, cpu));
 }
 
 extern void flush_smp_call_function_from_idle(void);
-- 
2.32.0.452.g940fe202ad


From f12dc7d91bde87663a537c37391b2f910d03fa12 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 10 Jul 2021 20:52:27 +0000
Subject: [PATCH 5/7] sched/alt: Remove sched_cpu_affinity_masks

---
 kernel/sched/alt_core.c | 44 ++++++++++++++++-------------------------
 1 file changed, 17 insertions(+), 27 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 38f2ef489b37..bb5f78a1e256 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -116,11 +116,9 @@ int sched_yield_type __read_mostly = 1;
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
-DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
-DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
-
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
+DEFINE_PER_CPU(cpumask_t *, sched_cpu_topo_end_mask);
 
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
@@ -891,8 +889,8 @@ int get_nohz_timer_target(void)
 		default_cpu = cpu;
 	}
 
-	for (mask = per_cpu(sched_cpu_affinity_masks, cpu) + 1;
-	     mask < per_cpu(sched_cpu_affinity_end_mask, cpu); mask++)
+	for (mask = per_cpu(sched_cpu_topo_masks, cpu) + 1;
+	     mask < per_cpu(sched_cpu_topo_end_mask, cpu); mask++)
 		for_each_cpu_and(i, mask, housekeeping_cpumask(HK_FLAG_TIMER))
 			if (!idle_cpu(i))
 				return i;
@@ -3932,7 +3930,7 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 
 static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 {
-	struct cpumask *affinity_mask, *end_mask;
+	struct cpumask *topo_mask, *end_mask;
 
 	if (unlikely(!rq->online))
 		return 0;
@@ -3940,11 +3938,11 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = per_cpu(sched_cpu_affinity_masks, cpu) + 1;
-	end_mask = per_cpu(sched_cpu_affinity_end_mask, cpu);
+	topo_mask = per_cpu(sched_cpu_topo_masks, cpu) + 1;
+	end_mask = per_cpu(sched_cpu_topo_end_mask, cpu);
 	do {
 		int i;
-		for_each_cpu_and(i, &sched_rq_pending_mask, affinity_mask) {
+		for_each_cpu_and(i, &sched_rq_pending_mask, topo_mask) {
 			int nr_migrated;
 			struct rq *src_rq;
 
@@ -3975,7 +3973,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 			spin_release(&src_rq->lock.dep_map, _RET_IP_);
 			do_raw_spin_unlock(&src_rq->lock);
 		}
-	} while (++affinity_mask < end_mask);
+	} while (++topo_mask < end_mask);
 
 	return 0;
 }
@@ -6637,14 +6635,6 @@ static void sched_init_topology_cpumask_early(void)
 	cpumask_t *tmp;
 
 	for_each_possible_cpu(cpu) {
-		/* init affinity masks */
-		tmp = per_cpu(sched_cpu_affinity_masks, cpu);
-
-		cpumask_copy(tmp, cpumask_of(cpu));
-		tmp++;
-		cpumask_copy(tmp, cpu_possible_mask);
-		cpumask_clear_cpu(cpu, tmp);
-		per_cpu(sched_cpu_affinity_end_mask, cpu) = ++tmp;
 		/* init topo masks */
 		tmp = per_cpu(sched_cpu_topo_masks, cpu);
 
@@ -6652,32 +6642,32 @@ static void sched_init_topology_cpumask_early(void)
 		tmp++;
 		cpumask_copy(tmp, cpu_possible_mask);
 		per_cpu(sched_cpu_llc_mask, cpu) = tmp;
+		per_cpu(sched_cpu_topo_end_mask, cpu) = ++tmp;
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
 	}
 }
 
-#define TOPOLOGY_CPUMASK(name, mask, last) \
-	if (cpumask_and(chk, chk, mask)) {					\
+#define TOPOLOGY_CPUMASK(name, mask, last)\
+	if (cpumask_and(topo, topo, mask)) {					\
 		cpumask_copy(topo, mask);					\
-		printk(KERN_INFO "sched: cpu#%02d affinity: 0x%08lx topo: 0x%08lx - "#name,\
-		       cpu, (chk++)->bits[0], (topo++)->bits[0]);		\
+		printk(KERN_INFO "sched: cpu#%02d topo: 0x%08lx - "#name,	\
+		       cpu, (topo++)->bits[0]);					\
 	}									\
 	if (!last)								\
-		cpumask_complement(chk, mask)
+		cpumask_complement(topo, mask)
 
 static void sched_init_topology_cpumask(void)
 {
 	int cpu;
-	cpumask_t *chk, *topo;
+	cpumask_t *topo;
 
 	for_each_online_cpu(cpu) {
 		/* take chance to reset time slice for idle tasks */
 		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
 
-		chk = per_cpu(sched_cpu_affinity_masks, cpu) + 1;
 		topo = per_cpu(sched_cpu_topo_masks, cpu) + 1;
 
-		cpumask_complement(chk, cpumask_of(cpu));
+		cpumask_complement(topo, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
@@ -6689,7 +6679,7 @@ static void sched_init_topology_cpumask(void)
 
 		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
 
-		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
+		per_cpu(sched_cpu_topo_end_mask, cpu) = topo;
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu),
 		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
-- 
2.32.0.452.g940fe202ad


From 9d9626a1d01434236648163acdb509d0808dd8ec Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 2 Aug 2021 13:56:40 +0800
Subject: [PATCH 6/7] sched/alt: kernel document update for sched_timeslice

---
 Documentation/admin-guide/kernel-parameters.txt | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 37192ffbd3f8..11e17f2f3a26 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -4879,9 +4879,9 @@
 	sbni=		[NET] Granch SBNI12 leased line adapter
 
 	sched_timeslice=
-			[KNL] Time slice in us for BMQ/PDS scheduler.
-			Format: <int> (must be >= 1000)
-			Default: 4000
+			[KNL] Time slice in ms for Project C BMQ/PDS scheduler.
+			Format: integer 2, 4
+			Default: 4
 			See Documentation/scheduler/sched-BMQ.txt
 
 	sched_verbose	[KNL] Enables verbose scheduler debug messages.
-- 
2.32.0.452.g940fe202ad


From 45d3558d56b759ffb872eccb78c8453cc6fdbab1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 29 Jul 2021 19:11:07 +0000
Subject: [PATCH 7/7] Project-C v5.13-r2

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index bb5f78a1e256..e296d56e85f0 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -67,7 +67,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define sched_feat(x)	(0)
 #endif /* CONFIG_SCHED_DEBUG */
 
-#define ALT_SCHED_VERSION "v5.13-r1"
+#define ALT_SCHED_VERSION "v5.13-r2"
 
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
-- 
2.32.0.452.g940fe202ad

