From 10ac88199aa43f8e4880933048734cdd846fba10 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Sat, 5 Mar 2022 19:37:08 +0100
Subject: [PATCH] LL: Implement ll-branding v5.16

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 drivers/cpufreq/cpufreq_ondemand.c | 9 +++++++++
 fs/dcache.c                        | 4 ++++
 include/linux/pagemap.h            | 4 ++++
 init/Kconfig                       | 4 ++++
 kernel/sched/alt_core.c            | 8 ++++++++
 kernel/sched/core.c                | 8 +++++++-
 mm/compaction.c                    | 6 +++++-
 mm/huge_memory.c                   | 4 ++++
 mm/page-writeback.c                | 8 ++++++++
 mm/vmscan.c                        | 8 ++++++++
 10 files changed, 61 insertions(+), 2 deletions(-)

diff --git a/drivers/cpufreq/cpufreq_ondemand.c b/drivers/cpufreq/cpufreq_ondemand.c
index 3b8f92477..704a933cb 100644
--- a/drivers/cpufreq/cpufreq_ondemand.c
+++ b/drivers/cpufreq/cpufreq_ondemand.c
@@ -18,10 +18,19 @@
 #include "cpufreq_ondemand.h"
 
 /* On-demand governor macros */
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_SCHED_ALT)
+#define DEF_FREQUENCY_UP_THRESHOLD		(55)
+#define DEF_SAMPLING_DOWN_FACTOR		(5)
+#else
 #define DEF_FREQUENCY_UP_THRESHOLD		(80)
 #define DEF_SAMPLING_DOWN_FACTOR		(1)
+#endif
 #define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_SCHED_ALT)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(63)
+#else
 #define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#endif
 #define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
 #define MIN_FREQUENCY_UP_THRESHOLD		(1)
 #define MAX_FREQUENCY_UP_THRESHOLD		(100)
diff --git a/fs/dcache.c b/fs/dcache.c
index 9508bd57a..4fd3ca183 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -71,7 +71,11 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_vfs_cache_pressure __read_mostly = 50;
+#else
 int sysctl_vfs_cache_pressure __read_mostly = 100;
+#endif
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index d150a9082..213f43109 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -996,7 +996,11 @@ struct readahead_control {
 		._index = i,						\
 	}
 
+#ifdef CONFIG_LL_BRANDING
+#define VM_READAHEAD_PAGES	(SZ_8M / PAGE_SIZE)
+#else
 #define VM_READAHEAD_PAGES	(SZ_128K / PAGE_SIZE)
+#endif
 
 void page_cache_ra_unbounded(struct readahead_control *,
 		unsigned long nr_to_read, unsigned long lookahead_count);
diff --git a/init/Kconfig b/init/Kconfig
index d783fe4a1..6dc0de0f8 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -107,6 +107,10 @@ config THREAD_INFO_IN_TASK
 
 menu "General setup"
 
+config LL_BRANDING
+	bool "Add Linux Lucjan branding"
+	default y
+
 config BROKEN
 	bool
 
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 4649c6dba..ca2d93ed1 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -77,7 +77,11 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
+#ifdef CONFIG_LL_BRANDING
+u64 sched_timeslice_ns __read_mostly = (2 << 20);
+#else
 u64 sched_timeslice_ns __read_mostly = (4 << 20);
+#endif
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
@@ -111,7 +115,11 @@ early_param("sched_timeslice", sched_timeslice);
  * 1: Deboost and requeue task. (default)
  * 2: Set rq skip task.
  */
+#ifdef CONFIG_LL_BRANDING
+int sched_yield_type __read_mostly = 2;
+#else
 int sched_yield_type __read_mostly = 1;
+#endif
 
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 64ca2a4e4..4937c4a97 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -74,7 +74,9 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
-#ifdef CONFIG_PREEMPT_RT
+#ifdef CONFIG_LL_BRANDING
+const_debug unsigned int sysctl_sched_nr_migrate = 256;
+#elif defined(CONFIG_PREEMPT_RT)
 const_debug unsigned int sysctl_sched_nr_migrate = 8;
 #else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
@@ -372,7 +374,11 @@ static inline void sched_core_dequeue(struct rq *rq, struct task_struct *p) { }
  * part of the period that we allow rt tasks to run in us.
  * default: 0.95s
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_sched_rt_runtime = 980000;
+#else
 int sysctl_sched_rt_runtime = 950000;
+#endif
 
 
 /*
diff --git a/mm/compaction.c b/mm/compaction.c
index 38f5e66a5..501b17b47 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1699,7 +1699,7 @@ typedef enum {
  * Allow userspace to control policy on scanning the unevictable LRU for
  * compactable pages.
  */
-#ifdef CONFIG_PREEMPT_RT
+#if defined(CONFIG_PREEMPT_RT) || defined(CONFIG_LL_BRANDING)
 int sysctl_compact_unevictable_allowed __read_mostly = 0;
 #else
 int sysctl_compact_unevictable_allowed __read_mostly = 1;
@@ -2704,7 +2704,11 @@ static void compact_nodes(void)
  * aggressively the kernel should compact memory in the
  * background. It takes values in the range [0, 100].
  */
+#ifdef CONFIG_LL_BRANDING
+unsigned int __read_mostly sysctl_compaction_proactiveness;
+#else
 unsigned int __read_mostly sysctl_compaction_proactiveness = 20;
+#endif
 
 int compaction_proactiveness_sysctl_handler(struct ctl_table *table, int write,
 		void *buffer, size_t *length, loff_t *ppos)
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 2e2ca7ecf..aa36a1704 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -54,7 +54,11 @@ unsigned long transparent_hugepage_flags __read_mostly =
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE
 	(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|
 #endif
+#ifdef CONFIG_LL_BRANDING
+	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG)|
+#else
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG)|
+#endif
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG)|
 	(1<<TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG);
 
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index a613f8ef6..0d541a578 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -70,7 +70,11 @@ static long ratelimit_pages = 32;
 /*
  * Start background writeback (via writeback threads) at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int dirty_background_ratio = 20;
+#else
 int dirty_background_ratio = 10;
+#endif
 
 /*
  * dirty_background_bytes starts at 0 (disabled) so that it is a function of
@@ -87,7 +91,11 @@ int vm_highmem_is_dirtyable;
 /*
  * The generator of dirty data starts writeback at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_dirty_ratio = 50;
+#else
 int vm_dirty_ratio = 20;
+#endif
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 619b08b82..dc6bcc06e 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -274,7 +274,11 @@ __setup("vm_unevictable_anon_kbytes_min=", vm_unevictable_anon_kbytes_min_setup)
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_swappiness = 30;
+#else
 int vm_swappiness = 60;
+#endif
 
 static void set_task_reclaim_state(struct task_struct *task,
 				   struct reclaim_state *rs)
@@ -4451,7 +4455,11 @@ static bool age_lruvec(struct lruvec *lruvec, struct scan_control *sc,
 }
 
 /* to protect the working set of the last N jiffies */
+#ifdef CONFIG_LL_BRANDING
+static unsigned long lru_gen_min_ttl __read_mostly = HZ;
+#else
 static unsigned long lru_gen_min_ttl __read_mostly;
+#endif
 
 static void lru_gen_age_node(struct pglist_data *pgdat, struct scan_control *sc)
 {
-- 
2.35.1.354.g715d08a9e5

