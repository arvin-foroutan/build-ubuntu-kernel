From 15aab38b94e24ab39c520deea090f4354adcc700 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Fri, 11 Mar 2022 02:52:30 +0100
Subject: [PATCH 1/8] sched/alt: Transpose the sched_rq_watermark array.

---
 kernel/sched/alt_core.c | 124 +++++++++++++++++++++++++++++++++-------
 1 file changed, 104 insertions(+), 20 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 6338a97b4..626e892bc 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -146,7 +146,87 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 #ifdef CONFIG_SCHED_SMT
 static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
-static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
+
+#define BITS_PER_ATOMIC_LONG_T BITS_PER_LONG
+typedef struct sched_bitmask {
+	atomic_long_t bits[DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T)];
+} sched_bitmask_t;
+static sched_bitmask_t sched_rq_watermark[NR_CPUS] ____cacheline_aligned_in_smp;
+
+#define x(p, set, mask)                                \
+	do {                                           \
+		if (set)                               \
+			atomic_long_or((mask), (p));   \
+		else                                   \
+			atomic_long_and(~(mask), (p)); \
+	} while (0)
+
+static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned int end,
+		unsigned int start, bool set)
+{
+	unsigned int start_idx, start_bit;
+	unsigned int end_idx, end_bit;
+	atomic_long_t *p;
+
+	if (end == start) {
+		return;
+	}
+
+	start_idx = start / BITS_PER_ATOMIC_LONG_T;
+	start_bit = start % BITS_PER_ATOMIC_LONG_T;
+	end_idx = (end - 1) / BITS_PER_ATOMIC_LONG_T;
+	end_bit = (end - 1) % BITS_PER_ATOMIC_LONG_T;
+	p = &sched_rq_watermark[cpu].bits[end_idx];
+
+	if (end_idx == start_idx) {
+		x(p, set, (~0UL >> (BITS_PER_ATOMIC_LONG_T - 1 - end_bit)) & (~0UL << start_bit));
+		return;
+	}
+
+	if (end_bit != BITS_PER_ATOMIC_LONG_T - 1) {
+		x(p, set, (~0UL >> (BITS_PER_ATOMIC_LONG_T - 1 - end_bit)));
+		p -= 1;
+		end_idx -= 1;
+	}
+
+	while (end_idx != start_idx) {
+		atomic_long_set(p, set ? ~0UL : 0);
+		p -= 1;
+		end_idx -= 1;
+	}
+
+	x(p, set, ~0UL << start_bit);
+}
+
+#undef x
+
+static __always_inline bool sched_rq_watermark_and(cpumask_t *dstp, const cpumask_t *cpus, int prio, bool not)
+{
+	int cpu;
+	bool ret = false;
+	int idx = prio / BITS_PER_ATOMIC_LONG_T;
+	int bit = prio % BITS_PER_ATOMIC_LONG_T;
+
+	cpumask_clear(dstp);
+	for_each_cpu(cpu, cpus)
+		if (test_bit(bit, (long*)&sched_rq_watermark[cpu].bits[idx].counter) == !not) {
+			__cpumask_set_cpu(cpu, dstp);
+			ret = true;
+		}
+	return ret;
+}
+
+static __always_inline bool sched_rq_watermark_test(const cpumask_t *cpus, int prio, bool not)
+{
+	int cpu;
+	int idx = prio / BITS_PER_ATOMIC_LONG_T;
+	int bit = prio % BITS_PER_ATOMIC_LONG_T;
+
+	for_each_cpu(cpu, cpus)
+		if (test_bit(bit, (long*)&sched_rq_watermark[cpu].bits[idx].counter) == !not)
+			return true;
+	return false;
+}
 
 /* sched_queue related functions */
 static inline void sched_queue_init(struct sched_queue *q)
@@ -175,7 +255,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 {
 	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
 	unsigned long last_wm = rq->watermark;
-	unsigned long i;
 	int cpu;
 
 	if (watermark == last_wm)
@@ -184,28 +263,25 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = last_wm; i > watermark; i--)
-			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+		sched_rq_watermark_fill_downwards(cpu, SCHED_BITS - 1 - watermark, SCHED_BITS - 1 - last_wm, false);
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
-		    IDLE_TASK_SCHED_PRIO == last_wm)
+		    unlikely(IDLE_TASK_SCHED_PRIO == last_wm))
 			cpumask_andnot(&sched_sg_idle_mask,
 				       &sched_sg_idle_mask, cpu_smt_mask(cpu));
 #endif
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = watermark; i > last_wm; i--)
-		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+	sched_rq_watermark_fill_downwards(cpu, SCHED_BITS - 1 - last_wm, SCHED_BITS - 1 - watermark, true);
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) &&
-	    IDLE_TASK_SCHED_PRIO == watermark) {
-		cpumask_t tmp;
+	    unlikely(IDLE_TASK_SCHED_PRIO == watermark)) {
+		const cpumask_t *smt_mask = cpu_smt_mask(cpu);
 
-		cpumask_and(&tmp, cpu_smt_mask(cpu), sched_rq_watermark);
-		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
+		if (!sched_rq_watermark_test(smt_mask, 0, true))
 			cpumask_or(&sched_sg_idle_mask,
-				   &sched_sg_idle_mask, cpu_smt_mask(cpu));
+				   &sched_sg_idle_mask, smt_mask);
 	}
 #endif
 }
@@ -1908,9 +1984,9 @@ static inline int select_task_rq(struct task_struct *p)
 #ifdef CONFIG_SCHED_SMT
 	    cpumask_and(&tmp, &chk_mask, &sched_sg_idle_mask) ||
 #endif
-	    cpumask_and(&tmp, &chk_mask, sched_rq_watermark) ||
-	    cpumask_and(&tmp, &chk_mask,
-			sched_rq_watermark + SCHED_BITS - task_sched_prio(p)))
+	    sched_rq_watermark_and(&tmp, &chk_mask, 0, false) ||
+	    sched_rq_watermark_and(&tmp, &chk_mask,
+			SCHED_BITS - task_sched_prio(p), false))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -3959,7 +4035,7 @@ static inline void sg_balance_check(struct rq *rq)
 	 * find potential cpus which can migrate the current running task
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
-	    cpumask_andnot(&chk, cpu_online_mask, sched_rq_watermark) &&
+	    sched_rq_watermark_and(&chk, cpu_online_mask, 0, true) &&
 	    cpumask_andnot(&chk, &chk, &sched_rq_pending_mask)) {
 		int i;
 
@@ -4267,9 +4343,8 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 #ifdef ALT_SCHED_DEBUG
 void alt_sched_debug(void)
 {
-	printk(KERN_INFO "sched: pending: 0x%04lx, idle: 0x%04lx, sg_idle: 0x%04lx\n",
+	printk(KERN_INFO "sched: pending: 0x%04lx, sg_idle: 0x%04lx\n",
 	       sched_rq_pending_mask.bits[0],
-	       sched_rq_watermark[0].bits[0],
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
@@ -7182,8 +7257,17 @@ void __init sched_init(void)
 	wait_bit_init();
 
 #ifdef CONFIG_SMP
-	for (i = 0; i < SCHED_BITS; i++)
-		cpumask_copy(sched_rq_watermark + i, cpu_present_mask);
+	for (i = 0; i < nr_cpu_ids; i++) {
+		long val = cpumask_test_cpu(i, cpu_present_mask) ? -1L : 0;
+		int j;
+		for (j = 0; j < DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T); j++)
+			atomic_long_set(&sched_rq_watermark[i].bits[j], val);
+	}
+	for (i = nr_cpu_ids; i < NR_CPUS; i++) {
+		int j;
+		for (j = 0; j < DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T); j++)
+			atomic_long_set(&sched_rq_watermark[i].bits[j], 0);
+	}
 #endif
 
 #ifdef CONFIG_CGROUP_SCHED
-- 
2.35.1.677.gabf474a5dd


From b30d999edcc8db6c13a55ba5edee3e6b0e0a15b8 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Tue, 15 Mar 2022 23:08:54 +0100
Subject: [PATCH 2/8] sched/alt: Add memory barriers around atomics.

---
 kernel/sched/alt_core.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 626e892bc..257eee65e 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -155,10 +155,12 @@ static sched_bitmask_t sched_rq_watermark[NR_CPUS] ____cacheline_aligned_in_smp;
 
 #define x(p, set, mask)                                \
 	do {                                           \
+		smp_mb__before_atomic();               \
 		if (set)                               \
 			atomic_long_or((mask), (p));   \
 		else                                   \
 			atomic_long_and(~(mask), (p)); \
+		smp_mb__after_atomic();                \
 	} while (0)
 
 static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned int end,
@@ -190,7 +192,9 @@ static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned
 	}
 
 	while (end_idx != start_idx) {
+		smp_mb__before_atomic();
 		atomic_long_set(p, set ? ~0UL : 0);
+		smp_mb__after_atomic();
 		p -= 1;
 		end_idx -= 1;
 	}
-- 
2.35.1.677.gabf474a5dd


From 1e16364f686ea499df0c4b1379137b8a889d3e93 Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Fri, 25 Mar 2022 09:37:48 +0100
Subject: [PATCH 3/8] alt_core.c: Potential fix for the UBSAN out-of-bounds
 warning

Link: https://gitlab.com/alfredchen/linux-prjc/-/merge_requests/11
---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 257eee65e..99acd34dc 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1990,7 +1990,7 @@ static inline int select_task_rq(struct task_struct *p)
 #endif
 	    sched_rq_watermark_and(&tmp, &chk_mask, 0, false) ||
 	    sched_rq_watermark_and(&tmp, &chk_mask,
-			SCHED_BITS - task_sched_prio(p), false))
+			SCHED_BITS - task_sched_prio(p) - 1, false))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
-- 
2.35.1.677.gabf474a5dd


From 8d6d3abb0d3aca5c2d0cc509cc3433b1a88b21a8 Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Fri, 18 Mar 2022 15:48:50 +0100
Subject: [PATCH 4/8] alt_core.c: Add potentially missing idle->on_rq
 assignment in init_idle()

---
 kernel/sched/alt_core.c | 1 +
 1 file changed, 1 insertion(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 99acd34dc..05e4df1f9 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -6693,6 +6693,7 @@ void __init init_idle(struct task_struct *idle, int cpu)
 
 	rq->idle = idle;
 	rcu_assign_pointer(rq->curr, idle);
+	idle->on_rq = TASK_ON_RQ_QUEUED;
 	idle->on_cpu = 1;
 
 	raw_spin_unlock(&rq->lock);
-- 
2.35.1.677.gabf474a5dd


From 13e695689774c3a00e18d064e5c510331bdcb1db Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Wed, 9 Mar 2022 14:03:08 +0100
Subject: [PATCH 5/8] alt_core.c: Add potentially missing assignment of
 p->on_cpu in sched_fork

---
 kernel/sched/alt_core.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 05e4df1f9..9c9dc1676 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -3066,6 +3066,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #ifdef CONFIG_SCHED_INFO
 	if (unlikely(sched_info_on()))
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
+#endif
+#if defined(CONFIG_SMP)
+	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
 
-- 
2.35.1.677.gabf474a5dd


From c8770fb2dcb80a8d4da3a16f92899311d8dcbf00 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Mon, 10 Jan 2022 02:26:08 +0100
Subject: [PATCH 6/8] sched/alt: [Sync] 61bb6cd2f765 mm: move
 node_reclaim_distance to fix NUMA without SMP

---
 kernel/sched/topology.c | 2 --
 1 file changed, 2 deletions(-)

diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index e5a7a638f..163cec668 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -2542,8 +2542,6 @@ void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 {}
 
 #ifdef CONFIG_NUMA
-int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
-
 int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 {
 	return best_mask_cpu(cpu, cpus);
-- 
2.35.1.677.gabf474a5dd


From bfef780701d56adc9fac4efaa0232560e0c78c90 Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Thu, 24 Feb 2022 11:09:22 +0100
Subject: [PATCH 7/8] init/Kconfig: [ProjectC]: Move uclamp config block out of
 SCHED_ALT menuconfig

---
 init/Kconfig | 58 ++++++++++++++++++++++++++--------------------------
 1 file changed, 29 insertions(+), 29 deletions(-)

diff --git a/init/Kconfig b/init/Kconfig
index 6be3308a3..2213c3060 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -814,35 +814,6 @@ config GENERIC_SCHED_CLOCK
 
 menu "Scheduler features"
 
-menuconfig SCHED_ALT
-	bool "Alternative CPU Schedulers"
-	default y
-	help
-	  This feature enable alternative CPU scheduler"
-
-if SCHED_ALT
-
-choice
-	prompt "Alternative CPU Scheduler"
-	default SCHED_BMQ
-
-config SCHED_BMQ
-	bool "BMQ CPU scheduler"
-	help
-	  The BitMap Queue CPU scheduler for excellent interactivity and
-	  responsiveness on the desktop and solid scalability on normal
-	  hardware and commodity servers.
-
-config SCHED_PDS
-	bool "PDS CPU scheduler"
-	help
-	  The Priority and Deadline based Skip list multiple queue CPU
-	  Scheduler.
-
-endchoice
-
-endif
-
 config UCLAMP_TASK
 	bool "Enable utilization clamping for RT/FAIR tasks"
 	depends on CPU_FREQ_GOV_SCHEDUTIL
@@ -893,6 +864,35 @@ config UCLAMP_BUCKETS_COUNT
 
 	  If in doubt, use the default value.
 
+menuconfig SCHED_ALT
+	bool "Alternative CPU Schedulers"
+	default y
+	help
+	  This feature enable alternative CPU scheduler"
+
+if SCHED_ALT
+
+choice
+	prompt "Alternative CPU Scheduler"
+	default SCHED_BMQ
+
+config SCHED_BMQ
+	bool "BMQ CPU scheduler"
+	help
+	  The BitMap Queue CPU scheduler for excellent interactivity and
+	  responsiveness on the desktop and solid scalability on normal
+	  hardware and commodity servers.
+
+config SCHED_PDS
+	bool "PDS CPU scheduler"
+	help
+	  The Priority and Deadline based Skip list multiple queue CPU
+	  Scheduler.
+
+endchoice
+
+endif
+
 endmenu
 
 #
-- 
2.35.1.677.gabf474a5dd


From 9999ea35399c55896bf92dc334f079b8c4913535 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Mon, 10 Jan 2022 00:16:19 +0100
Subject: [PATCH 8/8] sched/alt: [Sync] 32ed980c3020 sched: Remove unused
 inline function __rq_clock_broken()

---
 kernel/sched/alt_sched.h | 5 -----
 1 file changed, 5 deletions(-)

diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index f2b9e686d..6ff979a29 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -348,11 +348,6 @@ unsigned long arch_scale_freq_capacity(int cpu)
 }
 #endif
 
-static inline u64 __rq_clock_broken(struct rq *rq)
-{
-	return READ_ONCE(rq->clock);
-}
-
 static inline u64 rq_clock(struct rq *rq)
 {
 	/*
-- 
2.35.1.677.gabf474a5dd

